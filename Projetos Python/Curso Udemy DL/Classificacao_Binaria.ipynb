{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação Binária: Base Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>186.0000</td>\n",
       "      <td>275.0000</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>243.0000</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>173.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>198.00000</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>205.00000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>111.00000</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.45</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>141.00000</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>206.0000</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>144.00000</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.69</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>159.0000</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.98</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>277.00000</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>152.00000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.74</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>165.00000</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>265.0000</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>124.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9456.00</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      radius_mean   texture_mean   perimeter_mean   area_mean  \\\n",
       "0           17.99          10.38           122.80      1001.0   \n",
       "1           20.57          17.77           132.90      1326.0   \n",
       "2           19.69          21.25           130.00      1203.0   \n",
       "3           11.42          20.38            77.58       386.1   \n",
       "4           20.29          14.34           135.10      1297.0   \n",
       "..            ...            ...              ...         ...   \n",
       "564         21.56          22.39           142.00      1479.0   \n",
       "565         20.13          28.25           131.20      1261.0   \n",
       "566         16.60          28.08           108.30       858.1   \n",
       "567         20.60          29.33           140.10      1265.0   \n",
       "568          7.76          24.54            47.92       181.0   \n",
       "\n",
       "      smoothness_mean   compactness_mean   concavity_mean  \\\n",
       "0             0.11840            0.27760          0.30010   \n",
       "1             0.08474            0.07864          0.08690   \n",
       "2             0.10960            0.15990          0.19740   \n",
       "3             0.14250            0.28390          0.24140   \n",
       "4             0.10030            0.13280        198.00000   \n",
       "..                ...                ...              ...   \n",
       "564         111.00000            0.11590          0.24390   \n",
       "565           0.09780            0.10340        144.00000   \n",
       "566           0.08455            0.10230          0.09251   \n",
       "567           0.11780          277.00000          0.35140   \n",
       "568           0.05263            0.04362          0.00000   \n",
       "\n",
       "     concave_points_mean   symmetry_mean   fractal_dimension_mean  ...  \\\n",
       "0                0.14710          0.2419                  0.07871  ...   \n",
       "1                0.07017          0.1812                  0.05667  ...   \n",
       "2                0.12790          0.2069                  0.05999  ...   \n",
       "3                0.10520          0.2597                  0.09744  ...   \n",
       "4                0.10430          0.1809                  0.05883  ...   \n",
       "..                   ...             ...                      ...  ...   \n",
       "564              0.13890          0.1726                  0.05623  ...   \n",
       "565              0.09791          0.1752                  0.05533  ...   \n",
       "566              0.05302        159.0000                  0.05648  ...   \n",
       "567            152.00000          0.2397                  0.07016  ...   \n",
       "568              0.00000          0.1587                  0.05884  ...   \n",
       "\n",
       "      radius_worst   texture_worst   perimeter_worst   area_worst  \\\n",
       "0            25.38           17.33            184.60       2019.0   \n",
       "1            24.99           23.41            158.80       1956.0   \n",
       "2            23.57           25.53            152.50       1709.0   \n",
       "3            14.91           26.50             98.87        567.7   \n",
       "4            22.54           16.67            152.20       1575.0   \n",
       "..             ...             ...               ...          ...   \n",
       "564          25.45           26.40            166.10       2027.0   \n",
       "565          23.69           38.25            155.00       1731.0   \n",
       "566          18.98           34.12            126.70       1124.0   \n",
       "567          25.74           39.42            184.60       1821.0   \n",
       "568        9456.00           30.37             59.16        268.6   \n",
       "\n",
       "      smoothness_worst   compactness_worst   concavity_worst  \\\n",
       "0              0.16220             0.66560            0.7119   \n",
       "1              0.12380             0.18660            0.2416   \n",
       "2              0.14440             0.42450            0.4504   \n",
       "3              0.20980             0.86630            0.6869   \n",
       "4              0.13740           205.00000            0.4000   \n",
       "..                 ...                 ...               ...   \n",
       "564          141.00000             0.21130            0.4107   \n",
       "565            0.11660             0.19220            0.3215   \n",
       "566            0.11390             0.30940            0.3403   \n",
       "567          165.00000             0.86810            0.9387   \n",
       "568            0.08996             0.06444            0.0000   \n",
       "\n",
       "      concave_points_worst   symmetry_worst   fractal_dimension_worst  \n",
       "0                   0.2654           0.4601                   0.11890  \n",
       "1                 186.0000         275.0000                   0.08902  \n",
       "2                 243.0000           0.3613                   0.08758  \n",
       "3                   0.2575           0.6638                 173.00000  \n",
       "4                   0.1625           0.2364                   0.07678  \n",
       "..                     ...              ...                       ...  \n",
       "564                 0.2216         206.0000                   0.07115  \n",
       "565                 0.1628           0.2572                   0.06637  \n",
       "566                 0.1418           0.2218                   0.07820  \n",
       "567               265.0000           0.4087                 124.00000  \n",
       "568                 0.0000           0.2871                   0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsores = pd.read_csv(\"dados\\entradas_breast.csv\")\n",
    "previsores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "..  ..\n",
       "564  0\n",
       "565  0\n",
       "566  0\n",
       "567  0\n",
       "568  1\n",
       "\n",
       "[569 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = pd.read_csv('dados\\saidas_breast.csv')\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando a Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão entre base de teste e de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores_treinamento, previsores_teste, classes_treinamento, classes_teste = train_test_split(\n",
    "    previsores, classes, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt_entradas = previsores.shape[1]\n",
    "qt_saidas = 1\n",
    "qt_neuronios =  round((qt_entradas + qt_saidas)/2 )\n",
    "qt_neuronios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando camada oculta\n",
    "classificador.add( \n",
    "    Dense(\n",
    "        units = qt_neuronios,                  # Número de Neurônios da camada oculta\n",
    "        activation = 'relu',                   # Função de Ativação\n",
    "        kernel_initializer = 'random_uniform', # Kernel Inicial (random)\n",
    "        input_dim = qt_entradas                # Número de neurônios da camada de entrada\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando camada de saída\n",
    "classificador.add( \n",
    "    Dense(\n",
    "        units = 1,                             # Número de Neurônios da camada oculta\n",
    "        activation = 'sigmoid',                # Função de Ativação (sigmoide: ou 0 ou 1)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilando a RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador.compile(\n",
    "    optimizer= 'adam',               # Otimização da função de erro (poderia ser gradiente etc)\n",
    "    loss = 'binary_crossentropy',    # Maneira como calculamos o erro (neste caso entropia para binário)\n",
    "    metrics = ['binary_accuracy']    # Métricas \n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/43 [==============================] - 2s 6ms/step - loss: 9.2885 - binary_accuracy: 0.5352\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 2.1413 - binary_accuracy: 0.6479\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 1.0299 - binary_accuracy: 0.6995\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 1.0729 - binary_accuracy: 0.7394\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.8562 - binary_accuracy: 0.7559\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 1.2146 - binary_accuracy: 0.7606\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.6674 - binary_accuracy: 0.8451\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4715 - binary_accuracy: 0.8826\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4586 - binary_accuracy: 0.8709\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.5725 - binary_accuracy: 0.8592\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3365 - binary_accuracy: 0.9085\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3950 - binary_accuracy: 0.8709\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 1.0851 - binary_accuracy: 0.7958\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 1.1793 - binary_accuracy: 0.7981\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4424 - binary_accuracy: 0.8779\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.3894 - binary_accuracy: 0.8850\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3733 - binary_accuracy: 0.8732\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2681 - binary_accuracy: 0.8944\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3001 - binary_accuracy: 0.8826\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.2402 - binary_accuracy: 0.9202\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2141 - binary_accuracy: 0.9249\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2678 - binary_accuracy: 0.9155\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2423 - binary_accuracy: 0.9014\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2168 - binary_accuracy: 0.9272A: 0s - loss: 0.2277 - binary_accuracy: 0.91\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1917 - binary_accuracy: 0.9366\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2389 - binary_accuracy: 0.9061\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2013 - binary_accuracy: 0.9178\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1709 - binary_accuracy: 0.9296\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2815 - binary_accuracy: 0.9108\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3465 - binary_accuracy: 0.8991\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.5289 - binary_accuracy: 0.8732\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3643 - binary_accuracy: 0.9131\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2451 - binary_accuracy: 0.9061\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1748 - binary_accuracy: 0.9319\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1775 - binary_accuracy: 0.9343\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.5675 - binary_accuracy: 0.8568\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4585 - binary_accuracy: 0.8873\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2435 - binary_accuracy: 0.9249\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3778 - binary_accuracy: 0.8991\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2760 - binary_accuracy: 0.9131\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1845 - binary_accuracy: 0.9249\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2833 - binary_accuracy: 0.9085\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3260 - binary_accuracy: 0.8991\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1563 - binary_accuracy: 0.9390\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2518 - binary_accuracy: 0.9131\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2491 - binary_accuracy: 0.9272\n",
      "Epoch 47/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1785 - binary_accuracy: 0.9437\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.1594 - binary_accuracy: 0.9366\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2293 - binary_accuracy: 0.9131\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1484 - binary_accuracy: 0.9343\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3006 - binary_accuracy: 0.9085\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1844 - binary_accuracy: 0.9296\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1483 - binary_accuracy: 0.9531\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1407 - binary_accuracy: 0.9319\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1900 - binary_accuracy: 0.9390\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3689 - binary_accuracy: 0.9085\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3314 - binary_accuracy: 0.8991\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2178 - binary_accuracy: 0.9178\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2020 - binary_accuracy: 0.9296\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1253 - binary_accuracy: 0.9437\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1614 - binary_accuracy: 0.9319\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1941 - binary_accuracy: 0.9366\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2074 - binary_accuracy: 0.9343\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3230 - binary_accuracy: 0.9202\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1589 - binary_accuracy: 0.9413\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1318 - binary_accuracy: 0.9460\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1982 - binary_accuracy: 0.9131\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3323 - binary_accuracy: 0.8920\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4028 - binary_accuracy: 0.9085\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4931 - binary_accuracy: 0.8826\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3225 - binary_accuracy: 0.9085\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2583 - binary_accuracy: 0.9225\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1648 - binary_accuracy: 0.9390\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1240 - binary_accuracy: 0.9390\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1751 - binary_accuracy: 0.9460\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2882 - binary_accuracy: 0.9249\n",
      "Epoch 77/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2170 - binary_accuracy: 0.9390\n",
      "Epoch 78/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4508 - binary_accuracy: 0.8920\n",
      "Epoch 79/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2085 - binary_accuracy: 0.9366\n",
      "Epoch 80/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1960 - binary_accuracy: 0.9319\n",
      "Epoch 81/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1391 - binary_accuracy: 0.9390\n",
      "Epoch 82/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2472 - binary_accuracy: 0.9155\n",
      "Epoch 83/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2782 - binary_accuracy: 0.9014\n",
      "Epoch 84/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3730 - binary_accuracy: 0.9131\n",
      "Epoch 85/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4028 - binary_accuracy: 0.8920\n",
      "Epoch 86/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2872 - binary_accuracy: 0.9108\n",
      "Epoch 87/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2701 - binary_accuracy: 0.9249\n",
      "Epoch 88/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3433 - binary_accuracy: 0.9085\n",
      "Epoch 89/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1822 - binary_accuracy: 0.9460\n",
      "Epoch 90/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2478 - binary_accuracy: 0.9319\n",
      "Epoch 91/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2981 - binary_accuracy: 0.9131\n",
      "Epoch 92/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1452 - binary_accuracy: 0.9484\n",
      "Epoch 93/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.2053 - binary_accuracy: 0.9249\n",
      "Epoch 94/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1092 - binary_accuracy: 0.9624\n",
      "Epoch 95/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1457 - binary_accuracy: 0.9437\n",
      "Epoch 96/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1803 - binary_accuracy: 0.9507\n",
      "Epoch 97/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1255 - binary_accuracy: 0.9601\n",
      "Epoch 98/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.1408 - binary_accuracy: 0.9507\n",
      "Epoch 99/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1449 - binary_accuracy: 0.9484\n",
      "Epoch 100/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.1689 - binary_accuracy: 0.9413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29196107430>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador.fit(\n",
    "    previsores_treinamento, \n",
    "    classes_treinamento, \n",
    "    batch_size = 10,        # Calcula o erro para 10 registros e então atualiza\n",
    "    epochs = 100            # Número de épocas que iremos ajustar o peso\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsões e avaliando a RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.5773888e-01],\n",
       "       [5.8420557e-01],\n",
       "       [8.7259626e-01],\n",
       "       [9.3927503e-01],\n",
       "       [2.9995039e-01],\n",
       "       [1.1171392e-02],\n",
       "       [1.6812757e-05],\n",
       "       [1.9777078e-02],\n",
       "       [7.3359179e-01],\n",
       "       [5.8011614e-27],\n",
       "       [9.4205499e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.8931831e-01],\n",
       "       [1.8031217e-06],\n",
       "       [9.8853123e-01],\n",
       "       [2.5269711e-01],\n",
       "       [2.2922875e-04],\n",
       "       [9.9994850e-01],\n",
       "       [6.0901105e-01],\n",
       "       [2.1631305e-01],\n",
       "       [9.9986291e-01],\n",
       "       [9.4188166e-01],\n",
       "       [9.9948287e-01],\n",
       "       [7.8807688e-01],\n",
       "       [5.6632177e-07],\n",
       "       [9.9972683e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9971348e-01],\n",
       "       [1.7815158e-16],\n",
       "       [1.0000000e+00],\n",
       "       [1.3286113e-06],\n",
       "       [4.6614287e-03],\n",
       "       [9.7088808e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9878269e-01],\n",
       "       [9.9970680e-01],\n",
       "       [8.6204741e-12],\n",
       "       [6.7984863e-12],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [3.7698433e-04],\n",
       "       [9.9807274e-01],\n",
       "       [9.4867086e-01],\n",
       "       [1.7988308e-08],\n",
       "       [9.9971896e-01],\n",
       "       [7.2642534e-26],\n",
       "       [9.7393554e-01],\n",
       "       [9.5108300e-01],\n",
       "       [2.5205275e-03],\n",
       "       [6.7658544e-07],\n",
       "       [9.7413999e-01],\n",
       "       [1.0000000e+00],\n",
       "       [5.2227413e-05],\n",
       "       [6.2548929e-01],\n",
       "       [9.2936021e-01],\n",
       "       [7.7400109e-13],\n",
       "       [9.9695802e-01],\n",
       "       [1.0000000e+00],\n",
       "       [7.9366207e-02],\n",
       "       [4.3167612e-11],\n",
       "       [9.8800117e-01],\n",
       "       [1.0212091e-06],\n",
       "       [4.8010439e-28],\n",
       "       [3.4239566e-01],\n",
       "       [8.4566846e-02],\n",
       "       [4.9158119e-11],\n",
       "       [9.9948704e-01],\n",
       "       [9.9886703e-01],\n",
       "       [7.6965833e-01],\n",
       "       [1.0000000e+00],\n",
       "       [3.6981022e-03],\n",
       "       [9.9972695e-01],\n",
       "       [9.9969053e-01],\n",
       "       [7.0481986e-01],\n",
       "       [7.6169199e-05],\n",
       "       [3.6551419e-06],\n",
       "       [9.9692684e-01],\n",
       "       [6.6520476e-01],\n",
       "       [9.9959010e-01],\n",
       "       [1.0000000e+00],\n",
       "       [6.6579126e-10],\n",
       "       [9.9808586e-01],\n",
       "       [3.9361194e-03],\n",
       "       [2.6630187e-02],\n",
       "       [8.4108317e-01],\n",
       "       [1.0000000e+00],\n",
       "       [4.5901270e-06],\n",
       "       [4.0891027e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.2960166e-04],\n",
       "       [1.6675705e-04],\n",
       "       [1.9075359e-03],\n",
       "       [9.9906415e-01],\n",
       "       [9.9986649e-01],\n",
       "       [1.0000000e+00],\n",
       "       [4.5912554e-07],\n",
       "       [9.9999988e-01],\n",
       "       [9.8997843e-01],\n",
       "       [1.3603458e-12],\n",
       "       [9.9028254e-01],\n",
       "       [9.9957103e-01],\n",
       "       [9.9315083e-01],\n",
       "       [9.8791552e-01],\n",
       "       [1.9003000e-02],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [6.0260622e-04],\n",
       "       [9.8129845e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9172860e-01],\n",
       "       [4.3041177e-02],\n",
       "       [4.1563222e-01],\n",
       "       [1.4304709e-02],\n",
       "       [9.9425215e-01],\n",
       "       [8.6779815e-01],\n",
       "       [5.5583450e-03],\n",
       "       [9.9996173e-01],\n",
       "       [1.7636503e-13],\n",
       "       [2.0085877e-01],\n",
       "       [9.9955863e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.1612113e-01],\n",
       "       [2.0242760e-04],\n",
       "       [9.9971360e-01],\n",
       "       [9.8551244e-01],\n",
       "       [9.8973656e-01],\n",
       "       [5.3216350e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [5.3020742e-08],\n",
       "       [9.8269906e-05],\n",
       "       [2.0661767e-01],\n",
       "       [9.9644983e-01],\n",
       "       [5.3917826e-10],\n",
       "       [9.9914896e-01],\n",
       "       [1.0000000e+00],\n",
       "       [8.8603264e-01],\n",
       "       [1.2555605e-09],\n",
       "       [9.5904791e-01],\n",
       "       [9.9922526e-01],\n",
       "       [8.3272189e-02],\n",
       "       [9.9995387e-01],\n",
       "       [3.6301265e-07]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = classificador.predict(previsores_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = (previsoes > 0.5)\n",
    "previsoes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisao = accuracy_score(classes_teste, previsoes)\n",
    "precisao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 10],\n",
       "       [16, 75]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz = confusion_matrix(classes_teste, previsoes)\n",
    "matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6460 - binary_accuracy: 0.8182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6460208296775818, 0.8181818127632141]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados = classificador.evaluate(previsores_teste, classes_teste)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma RN com mais camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando camada oculta\n",
    "classificador2.add( \n",
    "    Dense(\n",
    "        units = qt_neuronios,                  # Número de Neurônios da camada oculta\n",
    "        activation = 'relu',                   # Função de Ativação\n",
    "        kernel_initializer = 'random_uniform', # Kernel Inicial (random)\n",
    "        input_dim = qt_entradas                # Número de neurônios da camada de entrada\n",
    "        )\n",
    "    )\n",
    "\n",
    "classificador2.add( \n",
    "    Dense(\n",
    "        units = qt_neuronios,                  # Número de Neurônios da camada oculta\n",
    "        activation = 'relu',                   # Função de Ativação\n",
    "        kernel_initializer = 'random_uniform', # Kernel (random)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando camada de saída\n",
    "classificador2.add( \n",
    "    Dense(\n",
    "        units = 1,                             # Número de Neurônios da camada oculta\n",
    "        activation = 'sigmoid',                # Função de Ativação (sigmoide: ou 0 ou 1)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizadores e modelando o classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "otimizador = tf.keras.optimizers.Adam(\n",
    "    lr =  0.001,    # Taxa de Aprendizagem\n",
    "    decay = 0.001,  # Decaimento\n",
    "    clipvalue = 0.5 # Evitar que fique preso\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador2.compile(\n",
    "    optimizer= otimizador,           # Otimização da função de erro (Adam personalizado)\n",
    "    loss = 'binary_crossentropy',    # Maneira como calculamos o erro (neste caso entropia para binário)\n",
    "    metrics = ['binary_accuracy']    # Métricas \n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/43 [==============================] - 1s 8ms/step - loss: 0.7044 - binary_accuracy: 0.7089\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.5027 - binary_accuracy: 0.7418\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4818 - binary_accuracy: 0.7981\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.5472 - binary_accuracy: 0.7746\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4929 - binary_accuracy: 0.8075\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.5111 - binary_accuracy: 0.7934\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4917 - binary_accuracy: 0.8192A: 0s - loss: 0.3157 - binary_accuracy: 0.\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4269 - binary_accuracy: 0.8568\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4222 - binary_accuracy: 0.8357\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.4110 - binary_accuracy: 0.8521\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4121 - binary_accuracy: 0.8474\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.4761 - binary_accuracy: 0.8521\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4187 - binary_accuracy: 0.8615\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4228 - binary_accuracy: 0.8615\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.4141 - binary_accuracy: 0.8545\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4076 - binary_accuracy: 0.8474\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4673 - binary_accuracy: 0.8380\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4522 - binary_accuracy: 0.8474\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3908 - binary_accuracy: 0.8427\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3886 - binary_accuracy: 0.8662\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3785 - binary_accuracy: 0.8662\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3764 - binary_accuracy: 0.8732\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3581 - binary_accuracy: 0.8709\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3806 - binary_accuracy: 0.8756\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3462 - binary_accuracy: 0.8779\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3810 - binary_accuracy: 0.8709\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3296 - binary_accuracy: 0.8779\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3645 - binary_accuracy: 0.8779\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3602 - binary_accuracy: 0.8685\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3796 - binary_accuracy: 0.8779\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3073 - binary_accuracy: 0.8991\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3114 - binary_accuracy: 0.8803\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3183 - binary_accuracy: 0.8873\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3444 - binary_accuracy: 0.8897\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4266 - binary_accuracy: 0.8756\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3399 - binary_accuracy: 0.8732\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3367 - binary_accuracy: 0.9014\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3532 - binary_accuracy: 0.9014\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.2976 - binary_accuracy: 0.8967\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3549 - binary_accuracy: 0.8850\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3182 - binary_accuracy: 0.8967\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3137 - binary_accuracy: 0.8850\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.2946 - binary_accuracy: 0.9108\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3218 - binary_accuracy: 0.8850\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3012 - binary_accuracy: 0.9014\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3164 - binary_accuracy: 0.8944\n",
      "Epoch 47/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3019 - binary_accuracy: 0.8967\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.2690 - binary_accuracy: 0.9108\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 0s 8ms/step - loss: 0.3120 - binary_accuracy: 0.9038\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.2906 - binary_accuracy: 0.8873: 0s - loss: 0.2965 - binary_accuracy: 0.887\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.3223 - binary_accuracy: 0.8803\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.2711 - binary_accuracy: 0.9131\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3294 - binary_accuracy: 0.9085\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.2842 - binary_accuracy: 0.9296\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.2976 - binary_accuracy: 0.9178\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3170 - binary_accuracy: 0.8826\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3173 - binary_accuracy: 0.9014\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2449 - binary_accuracy: 0.9272\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2772 - binary_accuracy: 0.9155\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3120 - binary_accuracy: 0.8967\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3000 - binary_accuracy: 0.9061\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3529 - binary_accuracy: 0.8944\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3836 - binary_accuracy: 0.9038\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 0s 4ms/step - loss: 0.3002 - binary_accuracy: 0.9038\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2959 - binary_accuracy: 0.9131\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3066 - binary_accuracy: 0.9108\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3195 - binary_accuracy: 0.9131\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3199 - binary_accuracy: 0.9085\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3251 - binary_accuracy: 0.9085\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3240 - binary_accuracy: 0.9202\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2974 - binary_accuracy: 0.9085\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3105 - binary_accuracy: 0.9108\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2944 - binary_accuracy: 0.9155\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3165 - binary_accuracy: 0.9225\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2965 - binary_accuracy: 0.9155\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3030 - binary_accuracy: 0.9178\n",
      "Epoch 77/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3082 - binary_accuracy: 0.9131\n",
      "Epoch 78/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3050 - binary_accuracy: 0.9131\n",
      "Epoch 79/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2859 - binary_accuracy: 0.9131\n",
      "Epoch 80/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2852 - binary_accuracy: 0.9061\n",
      "Epoch 81/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2862 - binary_accuracy: 0.9108\n",
      "Epoch 82/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.2953 - binary_accuracy: 0.9131\n",
      "Epoch 83/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3067 - binary_accuracy: 0.9061\n",
      "Epoch 84/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3080 - binary_accuracy: 0.9038\n",
      "Epoch 85/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3274 - binary_accuracy: 0.9131\n",
      "Epoch 86/100\n",
      "43/43 [==============================] - 0s 6ms/step - loss: 0.3443 - binary_accuracy: 0.8850\n",
      "Epoch 87/100\n",
      "43/43 [==============================] - 0s 7ms/step - loss: 0.3696 - binary_accuracy: 0.8991\n",
      "Epoch 88/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.4006 - binary_accuracy: 0.9038\n",
      "Epoch 89/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3213 - binary_accuracy: 0.8967\n",
      "Epoch 90/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3256 - binary_accuracy: 0.9155\n",
      "Epoch 91/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3078 - binary_accuracy: 0.9131\n",
      "Epoch 92/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3513 - binary_accuracy: 0.9085\n",
      "Epoch 93/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3483 - binary_accuracy: 0.8991\n",
      "Epoch 94/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3249 - binary_accuracy: 0.9225\n",
      "Epoch 95/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3102 - binary_accuracy: 0.9202\n",
      "Epoch 96/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3049 - binary_accuracy: 0.9131\n",
      "Epoch 97/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3939 - binary_accuracy: 0.9061\n",
      "Epoch 98/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3372 - binary_accuracy: 0.9131\n",
      "Epoch 99/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3862 - binary_accuracy: 0.9131\n",
      "Epoch 100/100\n",
      "43/43 [==============================] - 0s 5ms/step - loss: 0.3060 - binary_accuracy: 0.9249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x291b4f88d90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador2.fit(\n",
    "    previsores_treinamento, \n",
    "    classes_treinamento, \n",
    "    batch_size = 10,        # Calcula o erro para 10 registros e então atualiza\n",
    "    epochs = 100            # Número de épocas que iremos ajustar o peso\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando o classsificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = classificador2.predict(previsores_teste)\n",
    "previsoes = (previsoes > 0.5)\n",
    "len(previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8391608391608392"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisao = accuracy_score(classes_teste, previsoes)\n",
    "precisao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34, 18],\n",
       "       [ 5, 86]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz = confusion_matrix(classes_teste, previsoes)\n",
    "matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6069 - binary_accuracy: 0.8392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6069037914276123, 0.8391608595848083]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados = classificador2.evaluate(previsores_teste, classes_teste)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 9.04886648e-02,  4.57554013e-02,  3.79241332e-02,\n",
       "         -5.56839071e-03,  3.96894775e-02,  6.45722896e-02,\n",
       "         -4.01366055e-02, -1.31120929e-03, -2.02108249e-01,\n",
       "         -1.39628172e-01,  7.11857080e-02,  2.33164262e-02,\n",
       "         -4.16541733e-02, -3.08741238e-02, -5.71283884e-02,\n",
       "         -5.20443134e-02],\n",
       "        [ 4.37302887e-02, -6.27376363e-02,  3.82173248e-02,\n",
       "          8.81972089e-02,  6.54639527e-02, -4.36766334e-02,\n",
       "         -2.40502395e-02,  1.02405064e-01, -3.04560930e-01,\n",
       "         -1.52408689e-01,  1.80678070e-01,  7.71344155e-02,\n",
       "         -1.69061676e-01, -5.97392805e-02, -2.44848728e-02,\n",
       "         -3.87244932e-02],\n",
       "        [ 3.49527709e-02, -7.49632120e-02,  7.17263073e-02,\n",
       "         -2.63491496e-02,  3.32392938e-02, -1.23725587e-03,\n",
       "         -2.08095051e-02, -7.80927315e-02, -7.40634650e-02,\n",
       "         -1.52507856e-01,  5.22838533e-02,  2.74852160e-02,\n",
       "         -2.11926140e-02,  2.99004409e-02,  7.25475103e-02,\n",
       "          4.17947173e-02],\n",
       "        [-2.56088544e-02, -5.43303154e-02, -1.34768859e-02,\n",
       "          4.02102955e-02, -3.12874950e-02,  2.95136822e-03,\n",
       "          1.57419723e-02, -1.38525441e-02, -2.44247895e-02,\n",
       "         -2.36956105e-02, -3.49783525e-02, -5.05040102e-02,\n",
       "          5.68079241e-02,  5.48712537e-02,  4.91166823e-02,\n",
       "          5.20601235e-02],\n",
       "        [-1.43591259e-02,  8.62217508e-03, -7.74595607e-03,\n",
       "          4.04741168e-02,  6.69809952e-02,  1.19055972e-01,\n",
       "         -3.71890143e-02,  1.32385015e-01, -4.04800922e-02,\n",
       "         -1.49418086e-01, -9.39101502e-02,  1.38648257e-01,\n",
       "         -1.57906115e-01, -1.37091264e-01, -9.32133123e-02,\n",
       "          2.89716735e-03],\n",
       "        [-6.59277141e-02, -5.13501652e-02,  1.68394763e-02,\n",
       "         -4.47686762e-02, -2.76894439e-02,  7.20766857e-02,\n",
       "          1.84989255e-02,  6.96406234e-03,  4.51311059e-02,\n",
       "          4.65152040e-02,  2.84675770e-02, -5.72073013e-02,\n",
       "          1.07515447e-01,  9.25671682e-03, -1.34781422e-02,\n",
       "         -1.37889199e-03],\n",
       "        [-1.20801348e-02, -3.07523124e-02,  1.40589969e-02,\n",
       "         -1.84531268e-02,  8.04859698e-02,  3.78241017e-02,\n",
       "         -4.02671732e-02, -5.62364701e-03,  1.92493666e-02,\n",
       "          1.16575211e-01,  7.20230937e-02,  5.61831845e-03,\n",
       "         -5.89233153e-02,  3.27776559e-02, -1.22739784e-02,\n",
       "         -3.49367410e-02],\n",
       "        [ 5.20991301e-03, -1.24403164e-02,  4.59781848e-02,\n",
       "         -8.58599972e-03, -6.38880208e-02,  3.95685025e-02,\n",
       "         -4.16719653e-02,  1.68515425e-02,  1.01757631e-01,\n",
       "         -3.72262821e-02, -1.11284927e-01,  5.43664061e-02,\n",
       "         -3.36167850e-02, -4.22343500e-02, -9.66270864e-02,\n",
       "         -1.19426204e-02],\n",
       "        [-2.46520713e-02,  3.04013491e-02, -4.23846692e-02,\n",
       "         -6.95928633e-02,  5.93881458e-02, -1.56482402e-02,\n",
       "          1.83508508e-02, -5.65647036e-02,  5.65925464e-02,\n",
       "          4.10598051e-03, -9.51270480e-03, -5.86654479e-03,\n",
       "          1.72974970e-02,  3.30565460e-02,  4.73990925e-02,\n",
       "         -3.34306397e-02],\n",
       "        [ 1.92667544e-02,  3.01695545e-03,  1.20400690e-01,\n",
       "          7.72547647e-02, -9.70439166e-02, -6.89234510e-02,\n",
       "         -7.35295331e-03,  3.41042094e-02, -2.22733647e-01,\n",
       "         -2.26312857e-02,  5.58005199e-02,  1.65570304e-01,\n",
       "         -1.17712900e-01, -2.24034749e-02,  1.12051092e-01,\n",
       "          3.67365330e-01],\n",
       "        [ 7.35273063e-02, -3.98866087e-02,  4.63737221e-03,\n",
       "          8.15238357e-02, -5.67100430e-03, -1.90423019e-02,\n",
       "         -1.44995498e-02, -5.15194349e-02, -8.91632363e-02,\n",
       "         -1.49476916e-01, -8.93229805e-03,  2.52489243e-02,\n",
       "         -5.63504547e-03, -6.50779754e-02, -6.04257314e-03,\n",
       "          3.37119773e-02],\n",
       "        [ 6.48049347e-04,  8.07994418e-03,  4.95337835e-03,\n",
       "         -4.71956767e-02, -1.42080644e-02,  5.56081235e-02,\n",
       "         -1.46498047e-02,  4.05391678e-03,  1.14300139e-01,\n",
       "         -1.06171422e-01, -4.08875616e-03, -4.31248918e-03,\n",
       "          1.73095688e-02,  2.22672354e-02, -7.80037642e-02,\n",
       "         -2.29292847e-02],\n",
       "        [-3.77077842e-03,  1.34913651e-02, -6.84599113e-03,\n",
       "          3.48947905e-02, -4.31779213e-02,  1.86349582e-02,\n",
       "          7.10556703e-03,  5.10306433e-02,  3.27303298e-02,\n",
       "          1.14807934e-01,  2.32439898e-02, -1.46442326e-02,\n",
       "          5.69792325e-03,  5.56946546e-03,  5.91790900e-02,\n",
       "          7.30053429e-03],\n",
       "        [ 2.40866356e-02, -6.25023395e-02,  7.81635642e-02,\n",
       "          2.49808639e-01,  1.32479519e-01, -5.20047396e-02,\n",
       "         -2.12916896e-01,  2.17179701e-01, -4.72481906e-01,\n",
       "         -4.40967798e-01, -1.89377274e-03,  4.46356773e-01,\n",
       "         -3.44674438e-01, -3.88036638e-01, -3.27789634e-01,\n",
       "         -4.08592910e-01],\n",
       "        [ 2.63579376e-02, -3.41252126e-02,  5.08552678e-02,\n",
       "          9.78236124e-02,  2.07263455e-02, -9.06552672e-02,\n",
       "         -4.05892394e-02, -2.91120494e-03, -3.41767758e-01,\n",
       "         -9.85133424e-02,  1.67428970e-01,  1.07980222e-01,\n",
       "         -1.07445516e-01, -6.54957592e-02, -1.57761928e-02,\n",
       "         -7.59027526e-02],\n",
       "        [ 3.68247069e-02,  2.15050466e-02,  8.53330642e-03,\n",
       "          9.28792208e-02,  1.37399361e-02, -1.27842752e-02,\n",
       "         -3.68472263e-02,  2.12788850e-01, -1.19670689e-01,\n",
       "         -6.34561330e-02,  2.14190423e-01,  9.35760215e-02,\n",
       "         -5.51997349e-02, -1.38146266e-01, -1.13268159e-01,\n",
       "         -3.89899313e-01],\n",
       "        [ 1.01599485e-01, -5.01513369e-02,  8.30530226e-02,\n",
       "         -6.96188658e-02, -7.39549175e-02,  2.15973094e-01,\n",
       "          5.85014299e-02, -2.23663747e-01,  2.14470550e-01,\n",
       "         -2.30442677e-02,  1.47758290e-01,  6.15522414e-02,\n",
       "          2.22788174e-02,  4.13819738e-02,  2.05526039e-01,\n",
       "          1.65941715e-01],\n",
       "        [ 1.37704248e-02, -5.42074703e-02,  1.26119569e-01,\n",
       "         -3.88151146e-02,  4.92641423e-03, -2.42558956e-01,\n",
       "         -6.54286295e-02, -6.34242073e-02, -4.50879037e-01,\n",
       "         -9.04735550e-02,  2.01020781e-02,  7.80248269e-02,\n",
       "          6.54003583e-03,  4.05976474e-02,  8.19638148e-02,\n",
       "          1.57443240e-01],\n",
       "        [ 4.99440394e-02, -2.96314526e-03, -1.25644416e-01,\n",
       "         -1.96543291e-01, -1.42746344e-01,  7.92674571e-02,\n",
       "          7.10922405e-02, -2.57084936e-01, -3.28574836e-01,\n",
       "          1.93166047e-01,  5.88258021e-02, -2.50703126e-01,\n",
       "          1.49522290e-01,  1.58604994e-01,  2.52785981e-01,\n",
       "          1.96893856e-01],\n",
       "        [ 6.05015196e-02, -4.34517711e-02,  8.54026899e-02,\n",
       "          2.22009018e-01, -2.48901080e-02, -2.15833277e-01,\n",
       "         -6.24207705e-02,  1.88433856e-01, -3.68283421e-01,\n",
       "         -1.75924927e-01,  1.86850935e-01,  2.12214276e-01,\n",
       "         -1.78602695e-01, -1.46127865e-01, -1.15804136e-01,\n",
       "         -2.00403154e-01],\n",
       "        [-2.03217636e-03,  1.26142055e-01,  1.14932284e-02,\n",
       "          2.02903524e-02,  7.79014155e-02, -2.60022860e-02,\n",
       "         -9.41109434e-02,  1.33051714e-02, -2.52842098e-01,\n",
       "         -1.90763205e-01,  1.10614352e-01,  1.55912042e-01,\n",
       "         -1.21483788e-01, -4.03509997e-02, -1.73099339e-02,\n",
       "         -6.64594844e-02],\n",
       "        [ 3.83637734e-02,  3.84337059e-03, -2.74969963e-03,\n",
       "          1.57122150e-01,  1.08582661e-01, -1.50388375e-01,\n",
       "         -9.85707343e-02,  1.36323988e-01, -3.78071576e-01,\n",
       "         -1.63528383e-01,  1.59840479e-01,  1.20461300e-01,\n",
       "         -1.65608078e-01, -1.03438273e-01, -5.20327017e-02,\n",
       "         -1.58391014e-01],\n",
       "        [-1.68278106e-02, -5.36149442e-02,  4.92537348e-03,\n",
       "          1.96663234e-02,  2.10420787e-02,  8.63858778e-03,\n",
       "         -3.16624455e-02, -3.49439047e-02, -1.21807784e-01,\n",
       "         -1.41018689e-01,  2.14103214e-03,  6.99320063e-02,\n",
       "          8.24104343e-03, -2.60088593e-03,  1.84126049e-02,\n",
       "          2.89513293e-04],\n",
       "        [-1.63102224e-02, -1.71204023e-02, -2.11783382e-03,\n",
       "          1.15164623e-01,  7.13832155e-02, -4.39248793e-02,\n",
       "         -2.95568574e-02,  1.15209214e-01, -4.05173041e-02,\n",
       "         -1.71547700e-02, -5.26016951e-02,  2.14513745e-02,\n",
       "          1.67666003e-02, -1.04076434e-02, -1.94809679e-02,\n",
       "          3.08834948e-02],\n",
       "        [-1.75980181e-02,  8.94870982e-03, -8.64357576e-02,\n",
       "          2.47474983e-02,  4.40084487e-02, -1.08478747e-01,\n",
       "         -1.30035849e-02, -1.05006592e-02, -1.42516986e-01,\n",
       "          5.76470830e-02,  2.82571604e-03, -8.47708713e-03,\n",
       "         -1.05893649e-01, -7.48033151e-02,  8.70641395e-02,\n",
       "          2.60138884e-02],\n",
       "        [ 7.52697960e-02,  2.82959118e-02, -4.09181900e-02,\n",
       "         -9.64797661e-02, -1.80475246e-02, -2.72689555e-02,\n",
       "         -3.91003340e-02,  9.82159562e-03,  2.30276082e-02,\n",
       "         -1.17948145e-01, -6.28917813e-02, -3.06169260e-02,\n",
       "          7.03709722e-02, -2.19639838e-02,  3.09457295e-02,\n",
       "         -6.41048467e-03],\n",
       "        [-2.11538393e-02, -4.35347483e-02, -3.21338661e-02,\n",
       "         -1.16871595e-01,  6.39539957e-02, -1.59848556e-02,\n",
       "         -2.30869781e-02, -3.58329862e-02, -6.81954697e-02,\n",
       "         -6.01918958e-02, -3.48530672e-02,  9.42580998e-02,\n",
       "         -3.24534699e-02,  3.30156982e-02,  7.18320021e-03,\n",
       "         -7.08580308e-04],\n",
       "        [ 6.56392276e-02, -2.68530957e-02, -1.02927171e-01,\n",
       "          4.64458726e-02, -2.31148470e-02,  1.31742666e-02,\n",
       "         -1.12296291e-01,  1.01849496e-01, -5.59002459e-02,\n",
       "         -1.52921617e-01, -1.47307947e-01,  1.47089018e-02,\n",
       "         -1.04265675e-01, -1.40245184e-01, -9.12571996e-02,\n",
       "          1.64919998e-02],\n",
       "        [-2.50858143e-02, -6.03357144e-02, -6.97292760e-02,\n",
       "         -9.74227581e-03, -4.29245643e-03, -6.72889948e-02,\n",
       "         -2.49706972e-02, -1.13122471e-01, -1.16740458e-01,\n",
       "         -1.06994420e-01, -4.80552614e-02,  2.50901636e-02,\n",
       "          1.41030354e-02, -1.20393209e-01, -4.56659794e-02,\n",
       "          4.45494503e-02],\n",
       "        [ 6.39372766e-02, -7.67444447e-03, -5.16283512e-02,\n",
       "          2.08158106e-01,  1.64397359e-01, -8.82864967e-02,\n",
       "         -9.75820571e-02,  1.67672053e-01, -1.22337803e-01,\n",
       "         -7.35745877e-02,  7.77405575e-02, -5.64887822e-02,\n",
       "         -1.52037337e-01, -1.50180399e-01, -2.12217011e-02,\n",
       "         -2.34321043e-01]], dtype=float32),\n",
       " array([ 0.04527762, -0.01229406, -0.02719234, -0.1052509 , -0.0104347 ,\n",
       "         0.12171487,  0.03569134, -0.19657798, -0.11034145,  0.0011915 ,\n",
       "         0.11972562, -0.04570436,  0.03898752,  0.1262104 ,  0.17088602,\n",
       "         0.20073259], dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos0 = classificador2.layers[0].get_weights()\n",
    "pesos0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.40091572e-02, -1.06692843e-01,  5.49112111e-02,\n",
       "          2.64661200e-02, -1.24151886e-01, -6.07840791e-02,\n",
       "          3.69078815e-02,  4.95476760e-02,  6.86392039e-02,\n",
       "         -5.41867167e-02, -2.58370135e-02,  4.12755869e-02,\n",
       "         -1.11735538e-01,  3.23078670e-02, -3.43606025e-02,\n",
       "         -3.05315014e-02],\n",
       "        [ 2.53615645e-03, -3.18087824e-02,  4.30547521e-02,\n",
       "         -5.58837578e-02, -6.89312369e-02, -2.67014243e-02,\n",
       "         -4.73303981e-02, -3.88013162e-02, -2.86425445e-02,\n",
       "         -5.27621061e-02,  6.32949471e-02,  5.36689861e-03,\n",
       "         -2.13857405e-02, -1.06845200e-02, -2.96024960e-02,\n",
       "         -2.25502942e-02],\n",
       "        [-1.74336713e-02, -8.80967304e-02,  5.14031574e-02,\n",
       "          1.10370144e-02, -4.52729687e-02,  6.36889488e-02,\n",
       "         -9.23141930e-03,  2.39632782e-02, -1.83644667e-01,\n",
       "         -5.18035591e-02, -4.88636792e-02, -4.68644947e-02,\n",
       "         -9.97911021e-02, -6.45305263e-03, -5.28591052e-02,\n",
       "         -3.54164429e-02],\n",
       "        [ 1.03911944e-02,  5.79745993e-02,  1.69039853e-02,\n",
       "          8.37171450e-02, -6.17406629e-02,  4.20360155e-02,\n",
       "         -4.71179113e-02, -5.29217534e-02,  1.37513950e-02,\n",
       "          4.46773507e-02,  7.72512704e-02, -2.61266157e-02,\n",
       "          6.81420416e-02, -6.16536252e-02, -4.34915908e-02,\n",
       "         -3.91706079e-02],\n",
       "        [-4.34726551e-02, -4.89836968e-02,  2.28566378e-01,\n",
       "          2.52183899e-02, -7.68589601e-02, -2.34553218e-01,\n",
       "         -1.07451482e-02, -4.72824052e-02, -2.42794044e-02,\n",
       "         -1.74525902e-01,  2.31710710e-02, -6.04195194e-03,\n",
       "         -1.18065529e-01, -6.17230125e-02,  2.57206727e-02,\n",
       "         -1.07659891e-01],\n",
       "        [ 3.14941742e-02, -1.66287329e-02,  1.07679732e-01,\n",
       "         -4.65582311e-02, -8.53663012e-02,  1.09443247e-01,\n",
       "         -1.67732127e-03,  3.40657420e-02, -8.99985284e-02,\n",
       "         -3.77683528e-03,  4.50891443e-02, -6.27065450e-02,\n",
       "          1.37746856e-02, -6.19633049e-02, -2.70426236e-02,\n",
       "         -6.59262016e-03],\n",
       "        [ 3.13078687e-02, -5.11098374e-03,  9.74717177e-03,\n",
       "          1.53047945e-02,  1.14640193e-02,  1.49099082e-01,\n",
       "         -3.49132083e-02,  2.80727781e-02, -1.80278048e-02,\n",
       "         -2.97102034e-02, -6.23798370e-02, -6.68722950e-03,\n",
       "         -3.06732822e-02, -1.00825615e-02,  3.76666784e-02,\n",
       "          3.21414433e-02],\n",
       "        [-4.33667041e-02,  6.61091367e-03,  5.36381043e-02,\n",
       "          1.19620927e-01, -6.87671155e-02,  1.57403108e-02,\n",
       "         -4.71164137e-02, -1.53492009e-02,  4.47061248e-02,\n",
       "          6.72860956e-03, -6.52310904e-03,  5.41244671e-02,\n",
       "         -5.19944131e-02, -1.62287783e-02, -2.24459358e-02,\n",
       "         -3.46494317e-02],\n",
       "        [-6.47832733e-03,  1.04148705e-02,  1.10035785e-01,\n",
       "         -5.36391214e-02,  1.11211240e-01,  1.11782603e-01,\n",
       "          1.76812317e-02, -2.16937605e-02, -1.81153286e-02,\n",
       "         -1.10147838e-02, -1.22536290e-02, -4.87023816e-02,\n",
       "         -4.54068737e-04,  3.00243273e-02, -6.41134614e-03,\n",
       "          4.77825757e-03],\n",
       "        [-2.83385627e-02, -9.97891873e-02, -8.80067125e-02,\n",
       "         -5.95015176e-02,  8.34439620e-02, -2.30551045e-02,\n",
       "         -1.35091171e-02, -1.74749717e-02,  2.69855019e-02,\n",
       "         -5.70686758e-02, -6.95841312e-02,  3.06600183e-02,\n",
       "         -1.62436664e-02,  3.48767452e-02, -6.19282834e-02,\n",
       "          6.65365830e-02],\n",
       "        [ 2.61379108e-02, -1.99210420e-01, -6.11139387e-02,\n",
       "          9.34311897e-02,  1.37975356e-02,  1.98359653e-01,\n",
       "          8.21993686e-04, -5.24107367e-03,  3.45426053e-02,\n",
       "         -1.66241378e-01, -2.50580192e-01, -1.10861585e-02,\n",
       "         -1.16121829e-01, -1.33146066e-02,  2.96527892e-02,\n",
       "         -4.28273901e-02],\n",
       "        [-6.05990738e-02, -1.28727391e-01,  2.39296984e-02,\n",
       "          5.45403659e-02, -1.24078624e-01,  2.36327909e-02,\n",
       "          1.00710746e-02, -2.81242263e-02,  4.72431397e-03,\n",
       "         -7.70354643e-02,  1.07952677e-01, -2.74930499e-03,\n",
       "         -2.18004510e-02, -4.05472405e-02, -4.06627283e-02,\n",
       "         -2.80433539e-02],\n",
       "        [-3.63397002e-02,  1.22130942e-02,  1.27566373e-02,\n",
       "         -5.99156087e-03,  4.27668467e-02,  1.03993744e-01,\n",
       "          4.55741882e-02,  1.82945449e-02,  4.87503633e-02,\n",
       "         -2.11660936e-02, -7.73438141e-02,  5.43054193e-02,\n",
       "          9.86159872e-03, -2.75849178e-02, -5.24060801e-02,\n",
       "          4.05980945e-02],\n",
       "        [-2.67546661e-02,  2.32961252e-02, -3.04665766e-04,\n",
       "          3.58821601e-02,  6.35446087e-02,  3.86184640e-02,\n",
       "          1.36941608e-05, -7.99166039e-03, -5.41139208e-02,\n",
       "          1.20145157e-02, -5.48790358e-02, -4.78140712e-02,\n",
       "          5.08564524e-02, -7.84584321e-03,  2.87856255e-03,\n",
       "          6.59707040e-02],\n",
       "        [-3.90651859e-02,  2.67467815e-02, -7.63216540e-02,\n",
       "         -9.52538941e-03,  1.07412241e-01, -6.43614233e-02,\n",
       "         -2.39909673e-03, -1.64196026e-02, -8.30863789e-02,\n",
       "          9.40603539e-02,  9.12518054e-02, -7.43982866e-02,\n",
       "          4.56549563e-02,  5.05176652e-03,  4.08285409e-02,\n",
       "          6.87518269e-02],\n",
       "        [ 7.15255737e-07,  1.02528006e-01, -3.56332846e-02,\n",
       "         -7.98351988e-02,  6.37433454e-02, -1.18433170e-01,\n",
       "         -1.14417970e-02, -2.87718605e-02, -5.53091615e-02,\n",
       "          2.23641954e-02,  4.27608937e-02,  9.71459784e-03,\n",
       "          4.80611213e-02, -3.32573503e-02,  5.69717493e-04,\n",
       "          7.93974698e-02]], dtype=float32),\n",
       " array([-0.01318709,  0.21680334,  0.00813974, -0.14141901,  0.05669555,\n",
       "        -0.06940921,  0.0176966 , -0.0016054 , -0.18034816,  0.19501723,\n",
       "         0.24371539, -0.13043715,  0.21877995, -0.00404229,  0.01935541,\n",
       "         0.1354227 ], dtype=float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos1 = classificador2.layers[1].get_weights()\n",
    "pesos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.2170546 ],\n",
       "        [ 0.14623274],\n",
       "        [ 0.5638039 ],\n",
       "        [-0.5536045 ],\n",
       "        [ 0.42701966],\n",
       "        [-0.20805053],\n",
       "        [ 0.02942514],\n",
       "        [ 0.00271422],\n",
       "        [-0.6091747 ],\n",
       "        [ 0.17441411],\n",
       "        [ 0.16986017],\n",
       "        [-0.489872  ],\n",
       "        [ 0.21767426],\n",
       "        [-0.53436637],\n",
       "        [ 0.06699091],\n",
       "        [ 0.34902167]], dtype=float32),\n",
       " array([0.12839694], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos2 = classificador2.layers[2].get_weights()\n",
    "pesos2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(29/08: parei aqui)\n",
    "\n",
    "O array separado se refere a camada de bias, que não se relaciona com a rede neural em si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    classificador3 = Sequential()\n",
    "    # Criando camada oculta\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = qt_neuronios,                  # Número de Neurônios da camada oculta\n",
    "            activation = 'relu',                   # Função de Ativação\n",
    "            kernel_initializer = 'random_uniform', # Kernel Inicial (random)\n",
    "            input_dim = qt_entradas                # Número de neurônios da camada de entrada\n",
    "        )\n",
    "    )\n",
    "\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = qt_neuronios,                  # Número de Neurônios da camada oculta\n",
    "            activation = 'relu',                   # Função de Ativação\n",
    "            kernel_initializer = 'random_uniform', # Kernel (random)\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    # Criando camada de saída\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = 1,                             # Número de Neurônios da saida\n",
    "            activation = 'sigmoid',                # Função de Ativação (sigmoide: ou 0 ou 1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    otimizador = tf.keras.optimizers.Adam(\n",
    "        lr =  0.001,    # Taxa de Aprendizagem\n",
    "        decay = 0.001,  # Decaimento\n",
    "        clipvalue = 0.5 # Evitar que fique preso\n",
    "    )\n",
    "\n",
    "    classificador3.compile(\n",
    "        optimizer= otimizador,           # Otimização da função de erro (Adam personalizado)\n",
    "        loss = 'binary_crossentropy',    # Maneira como calculamos o erro (neste caso entropia para binário)\n",
    "        metrics = ['binary_accuracy']    # Métricas \n",
    "    )  \n",
    "    \n",
    "    return classificador3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador = KerasClassifier(\n",
    "    build_fn= criarRede,\n",
    "    epochs = 100,\n",
    "    batch_size = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 8ms/step - loss: 0.9734 - binary_accuracy: 0.6465\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5069 - binary_accuracy: 0.7539A: 0s - loss: 0.5062 - binary_accuracy: 0.754\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5209 - binary_accuracy: 0.7598\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5906 - binary_accuracy: 0.7617\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6243 - binary_accuracy: 0.7754\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5268 - binary_accuracy: 0.7988\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4540 - binary_accuracy: 0.8125\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4048 - binary_accuracy: 0.8379\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4450 - binary_accuracy: 0.8242\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4382 - binary_accuracy: 0.8359\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.3883 - binary_accuracy: 0.8535- ETA: 0s - loss: 0.2654 - binary_accuracy: 0. - 0s 5ms/step - loss: 0.3841 - binary_accuracy: 0.8555\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3756 - binary_accuracy: 0.8438\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3386 - binary_accuracy: 0.8691\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3533 - binary_accuracy: 0.8730\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3591 - binary_accuracy: 0.8789A: 0s - loss: 0.4262 - binary_accuracy: 0.\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3122 - binary_accuracy: 0.8750\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3682 - binary_accuracy: 0.8730\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3213 - binary_accuracy: 0.8867\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3395 - binary_accuracy: 0.8691\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3477 - binary_accuracy: 0.8770\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3531 - binary_accuracy: 0.8828\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3485 - binary_accuracy: 0.8906\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3508 - binary_accuracy: 0.8945\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3076 - binary_accuracy: 0.8848\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3030 - binary_accuracy: 0.8867\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3056 - binary_accuracy: 0.9043\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3156 - binary_accuracy: 0.8887\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2843 - binary_accuracy: 0.9062\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2742 - binary_accuracy: 0.9121\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2725 - binary_accuracy: 0.8965\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3212 - binary_accuracy: 0.8926\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3002 - binary_accuracy: 0.9180\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3362 - binary_accuracy: 0.8828\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3327 - binary_accuracy: 0.8887\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2739 - binary_accuracy: 0.9004\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2924 - binary_accuracy: 0.9062\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3015 - binary_accuracy: 0.9043\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3062 - binary_accuracy: 0.9141\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2704 - binary_accuracy: 0.9121\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3133 - binary_accuracy: 0.8965\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2762 - binary_accuracy: 0.9004\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2796 - binary_accuracy: 0.9141\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3066 - binary_accuracy: 0.9043\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2720 - binary_accuracy: 0.9102\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3112 - binary_accuracy: 0.9062\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3346 - binary_accuracy: 0.9023\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2600 - binary_accuracy: 0.9043\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2868 - binary_accuracy: 0.8945\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2675 - binary_accuracy: 0.9043\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2470 - binary_accuracy: 0.9043\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2565 - binary_accuracy: 0.9180\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2872 - binary_accuracy: 0.9102\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2871 - binary_accuracy: 0.9062\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2911 - binary_accuracy: 0.9082\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2696 - binary_accuracy: 0.9102\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2720 - binary_accuracy: 0.9062\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2521 - binary_accuracy: 0.9258\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2586 - binary_accuracy: 0.9121\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2886 - binary_accuracy: 0.8984\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2596 - binary_accuracy: 0.9199\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2915 - binary_accuracy: 0.9004\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2414 - binary_accuracy: 0.9258\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2902 - binary_accuracy: 0.9082\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2666 - binary_accuracy: 0.9160\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2955 - binary_accuracy: 0.9238\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2827 - binary_accuracy: 0.9219\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2840 - binary_accuracy: 0.9004\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2833 - binary_accuracy: 0.9043\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2979 - binary_accuracy: 0.9062\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3126 - binary_accuracy: 0.8945\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3013 - binary_accuracy: 0.9180\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2725 - binary_accuracy: 0.9062\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2718 - binary_accuracy: 0.9043\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2903 - binary_accuracy: 0.9082\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2703 - binary_accuracy: 0.9141\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2756 - binary_accuracy: 0.9102\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2732 - binary_accuracy: 0.9082\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2936 - binary_accuracy: 0.9102\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3027 - binary_accuracy: 0.9004\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2689 - binary_accuracy: 0.9180\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2733 - binary_accuracy: 0.9199\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2895 - binary_accuracy: 0.9219\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3115 - binary_accuracy: 0.9121\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2816 - binary_accuracy: 0.9102\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2986 - binary_accuracy: 0.9121\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3282 - binary_accuracy: 0.8906\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3305 - binary_accuracy: 0.9141\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3051 - binary_accuracy: 0.9062\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2916 - binary_accuracy: 0.9043\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3091 - binary_accuracy: 0.9004\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2957 - binary_accuracy: 0.9004\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3169 - binary_accuracy: 0.9102\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3055 - binary_accuracy: 0.9043\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2874 - binary_accuracy: 0.8984\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3489 - binary_accuracy: 0.9043\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3027 - binary_accuracy: 0.9082\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3287 - binary_accuracy: 0.9004\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2893 - binary_accuracy: 0.9141\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3124 - binary_accuracy: 0.8965\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3054 - binary_accuracy: 0.9004\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 4ms/step - loss: 0.7965 - binary_accuracy: 0.6309\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5462 - binary_accuracy: 0.6797\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4898 - binary_accuracy: 0.7344\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4480 - binary_accuracy: 0.7793\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4351 - binary_accuracy: 0.7891\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4118 - binary_accuracy: 0.8398\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4048 - binary_accuracy: 0.8262\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3899 - binary_accuracy: 0.8379\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3817 - binary_accuracy: 0.8496\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3725 - binary_accuracy: 0.8535\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3548 - binary_accuracy: 0.8555\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3751 - binary_accuracy: 0.8457\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3352 - binary_accuracy: 0.8594\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3254 - binary_accuracy: 0.8770\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3360 - binary_accuracy: 0.8848\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3021 - binary_accuracy: 0.8730\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3077 - binary_accuracy: 0.8809\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2985 - binary_accuracy: 0.8809\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3540 - binary_accuracy: 0.8672\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3068 - binary_accuracy: 0.8789\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2972 - binary_accuracy: 0.8691\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2861 - binary_accuracy: 0.8691\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2621 - binary_accuracy: 0.8926\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2842 - binary_accuracy: 0.8945\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3005 - binary_accuracy: 0.8770\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2641 - binary_accuracy: 0.8945\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2757 - binary_accuracy: 0.8984\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2876 - binary_accuracy: 0.8828\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2803 - binary_accuracy: 0.9023\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2702 - binary_accuracy: 0.8848\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2849 - binary_accuracy: 0.8867\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2720 - binary_accuracy: 0.9004\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2684 - binary_accuracy: 0.9043\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2489 - binary_accuracy: 0.9102\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3256 - binary_accuracy: 0.8809\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2605 - binary_accuracy: 0.8945\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2558 - binary_accuracy: 0.8984\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2911 - binary_accuracy: 0.8965\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3427 - binary_accuracy: 0.8984\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2574 - binary_accuracy: 0.9062\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2943 - binary_accuracy: 0.8848\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2487 - binary_accuracy: 0.9160\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2482 - binary_accuracy: 0.9023\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2612 - binary_accuracy: 0.9160\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2326 - binary_accuracy: 0.9004\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2570 - binary_accuracy: 0.8984\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2273 - binary_accuracy: 0.9141\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2594 - binary_accuracy: 0.8984\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2432 - binary_accuracy: 0.9141\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2744 - binary_accuracy: 0.9043\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3162 - binary_accuracy: 0.9062\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2511 - binary_accuracy: 0.9062\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2489 - binary_accuracy: 0.9043\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2240 - binary_accuracy: 0.9102\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2096 - binary_accuracy: 0.9160\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2114 - binary_accuracy: 0.9199\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2082 - binary_accuracy: 0.9219\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2736 - binary_accuracy: 0.9023\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2888 - binary_accuracy: 0.9004\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2581 - binary_accuracy: 0.9082\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2456 - binary_accuracy: 0.9141\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2979 - binary_accuracy: 0.8984\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2302 - binary_accuracy: 0.9121\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2411 - binary_accuracy: 0.9180\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2450 - binary_accuracy: 0.9062\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3145 - binary_accuracy: 0.8828\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2654 - binary_accuracy: 0.9062\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2492 - binary_accuracy: 0.9219\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2401 - binary_accuracy: 0.9062\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2145 - binary_accuracy: 0.9160\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2478 - binary_accuracy: 0.8984\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2548 - binary_accuracy: 0.9160\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2267 - binary_accuracy: 0.9062\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2862 - binary_accuracy: 0.9004\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2751 - binary_accuracy: 0.9082\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2180 - binary_accuracy: 0.9160\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2249 - binary_accuracy: 0.9219\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2247 - binary_accuracy: 0.9199\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2467 - binary_accuracy: 0.9160\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2665 - binary_accuracy: 0.9062\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2294 - binary_accuracy: 0.9238\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2334 - binary_accuracy: 0.9102\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2454 - binary_accuracy: 0.9121\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2583 - binary_accuracy: 0.9102\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2242 - binary_accuracy: 0.9102\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2307 - binary_accuracy: 0.9219\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2412 - binary_accuracy: 0.9062\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2786 - binary_accuracy: 0.9062\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2304 - binary_accuracy: 0.9121\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2542 - binary_accuracy: 0.9062\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2506 - binary_accuracy: 0.9199\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2248 - binary_accuracy: 0.9102\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2530 - binary_accuracy: 0.9160\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2586 - binary_accuracy: 0.9160\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2294 - binary_accuracy: 0.9102\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2786 - binary_accuracy: 0.9043\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2794 - binary_accuracy: 0.9082\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2508 - binary_accuracy: 0.9141\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2365 - binary_accuracy: 0.9180\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2805 - binary_accuracy: 0.9082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 4ms/step - loss: 0.6185 - binary_accuracy: 0.6562\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5026 - binary_accuracy: 0.7285\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4712 - binary_accuracy: 0.7715\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4218 - binary_accuracy: 0.8027\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4264 - binary_accuracy: 0.8242\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3917 - binary_accuracy: 0.8242\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4514 - binary_accuracy: 0.8418\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4262 - binary_accuracy: 0.8398\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3566 - binary_accuracy: 0.8438\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3967 - binary_accuracy: 0.8340\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3467 - binary_accuracy: 0.8672\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5163 - binary_accuracy: 0.8223\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4298 - binary_accuracy: 0.8457\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4309 - binary_accuracy: 0.8418\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4630 - binary_accuracy: 0.8477\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4036 - binary_accuracy: 0.8379\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4033 - binary_accuracy: 0.8809\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4547 - binary_accuracy: 0.8457\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3980 - binary_accuracy: 0.8652\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4493 - binary_accuracy: 0.8516\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4468 - binary_accuracy: 0.8555\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3752 - binary_accuracy: 0.8691\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3585 - binary_accuracy: 0.8867\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4299 - binary_accuracy: 0.8359\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4051 - binary_accuracy: 0.8652\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3851 - binary_accuracy: 0.8496\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3425 - binary_accuracy: 0.8750\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3255 - binary_accuracy: 0.8887\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3486 - binary_accuracy: 0.8867\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3905 - binary_accuracy: 0.8535\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3401 - binary_accuracy: 0.8750\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3482 - binary_accuracy: 0.8711\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3896 - binary_accuracy: 0.8613\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3845 - binary_accuracy: 0.8672\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3412 - binary_accuracy: 0.8848\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4190 - binary_accuracy: 0.8633\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3274 - binary_accuracy: 0.9062\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3130 - binary_accuracy: 0.8887\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3415 - binary_accuracy: 0.8848\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3124 - binary_accuracy: 0.9023\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2851 - binary_accuracy: 0.9082\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3002 - binary_accuracy: 0.8828\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2901 - binary_accuracy: 0.9062\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2838 - binary_accuracy: 0.9102\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2853 - binary_accuracy: 0.9023\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2841 - binary_accuracy: 0.9062\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2753 - binary_accuracy: 0.8984\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2831 - binary_accuracy: 0.8906\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2832 - binary_accuracy: 0.8984\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3196 - binary_accuracy: 0.8867\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3052 - binary_accuracy: 0.8789\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2786 - binary_accuracy: 0.9121\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2890 - binary_accuracy: 0.8887\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2704 - binary_accuracy: 0.9023\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2777 - binary_accuracy: 0.9102\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2880 - binary_accuracy: 0.9023\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3224 - binary_accuracy: 0.8887\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2807 - binary_accuracy: 0.9023\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2737 - binary_accuracy: 0.8945\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2823 - binary_accuracy: 0.9023\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2860 - binary_accuracy: 0.8945\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2772 - binary_accuracy: 0.9043\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2902 - binary_accuracy: 0.8945\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2778 - binary_accuracy: 0.9121\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2732 - binary_accuracy: 0.9141\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2737 - binary_accuracy: 0.9102\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2675 - binary_accuracy: 0.9062\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3010 - binary_accuracy: 0.8984\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2588 - binary_accuracy: 0.9102\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2621 - binary_accuracy: 0.9102\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2720 - binary_accuracy: 0.9121\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2693 - binary_accuracy: 0.9062\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2594 - binary_accuracy: 0.9062\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2646 - binary_accuracy: 0.9160\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2677 - binary_accuracy: 0.9004\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2757 - binary_accuracy: 0.9043\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2901 - binary_accuracy: 0.9023\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2851 - binary_accuracy: 0.8945\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2733 - binary_accuracy: 0.9121\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2979 - binary_accuracy: 0.8906\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2785 - binary_accuracy: 0.9160\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2750 - binary_accuracy: 0.9043\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2625 - binary_accuracy: 0.9082\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2651 - binary_accuracy: 0.8984\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2880 - binary_accuracy: 0.9043\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2884 - binary_accuracy: 0.8867\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2655 - binary_accuracy: 0.9062\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2861 - binary_accuracy: 0.9004\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2834 - binary_accuracy: 0.8926\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2676 - binary_accuracy: 0.9062\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2664 - binary_accuracy: 0.9121\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2796 - binary_accuracy: 0.9102\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2744 - binary_accuracy: 0.9082\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2729 - binary_accuracy: 0.9121\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2789 - binary_accuracy: 0.8984\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2779 - binary_accuracy: 0.9102\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2920 - binary_accuracy: 0.9023\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2937 - binary_accuracy: 0.8906\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2974 - binary_accuracy: 0.8926\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2981 - binary_accuracy: 0.9062\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 4ms/step - loss: 0.9591 - binary_accuracy: 0.6738\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5593 - binary_accuracy: 0.7305\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5148 - binary_accuracy: 0.8008\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4440 - binary_accuracy: 0.8223\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4395 - binary_accuracy: 0.8320\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5399 - binary_accuracy: 0.8066\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4303 - binary_accuracy: 0.8242\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4134 - binary_accuracy: 0.8359\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4121 - binary_accuracy: 0.8574\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3726 - binary_accuracy: 0.8750\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4396 - binary_accuracy: 0.8477\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3900 - binary_accuracy: 0.8613\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3906 - binary_accuracy: 0.8652\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4869 - binary_accuracy: 0.8516\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3613 - binary_accuracy: 0.8750\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4451 - binary_accuracy: 0.8535\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3941 - binary_accuracy: 0.8535\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3938 - binary_accuracy: 0.8789\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4126 - binary_accuracy: 0.8516\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3613 - binary_accuracy: 0.8770\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3999 - binary_accuracy: 0.8672\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3707 - binary_accuracy: 0.8613\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3912 - binary_accuracy: 0.8535\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4571 - binary_accuracy: 0.8418\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4399 - binary_accuracy: 0.8594\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3769 - binary_accuracy: 0.8652\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3892 - binary_accuracy: 0.8594\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3793 - binary_accuracy: 0.8926\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3767 - binary_accuracy: 0.8828\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4049 - binary_accuracy: 0.8496\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4269 - binary_accuracy: 0.8887\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3780 - binary_accuracy: 0.8652\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3495 - binary_accuracy: 0.8887\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3913 - binary_accuracy: 0.8711\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3414 - binary_accuracy: 0.8848\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3257 - binary_accuracy: 0.8848\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3171 - binary_accuracy: 0.8945\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3508 - binary_accuracy: 0.8906\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3579 - binary_accuracy: 0.8848\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3616 - binary_accuracy: 0.9023\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3161 - binary_accuracy: 0.8945\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3236 - binary_accuracy: 0.8984\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3348 - binary_accuracy: 0.9004\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3666 - binary_accuracy: 0.8906\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3188 - binary_accuracy: 0.8750\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3637 - binary_accuracy: 0.8691\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3325 - binary_accuracy: 0.8926\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3215 - binary_accuracy: 0.9082\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3223 - binary_accuracy: 0.9141\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3101 - binary_accuracy: 0.8867\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3726 - binary_accuracy: 0.8984\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3135 - binary_accuracy: 0.8926\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3390 - binary_accuracy: 0.8965\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3194 - binary_accuracy: 0.9062\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3032 - binary_accuracy: 0.9062\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2994 - binary_accuracy: 0.9062\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3511 - binary_accuracy: 0.9004\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3758 - binary_accuracy: 0.8809\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3118 - binary_accuracy: 0.9141\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3368 - binary_accuracy: 0.8965\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3559 - binary_accuracy: 0.9004\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2969 - binary_accuracy: 0.9023\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3343 - binary_accuracy: 0.8945\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3069 - binary_accuracy: 0.9062\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2970 - binary_accuracy: 0.9082\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3495 - binary_accuracy: 0.8945\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4048 - binary_accuracy: 0.8770\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3098 - binary_accuracy: 0.9004\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3047 - binary_accuracy: 0.9102\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3166 - binary_accuracy: 0.9004\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2958 - binary_accuracy: 0.9141\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3410 - binary_accuracy: 0.8945\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3042 - binary_accuracy: 0.9082\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3214 - binary_accuracy: 0.9043\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2868 - binary_accuracy: 0.9102\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2718 - binary_accuracy: 0.9258\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2831 - binary_accuracy: 0.9121\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3000 - binary_accuracy: 0.9121\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2958 - binary_accuracy: 0.9082\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2945 - binary_accuracy: 0.9082\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2742 - binary_accuracy: 0.9199\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2804 - binary_accuracy: 0.9258\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2721 - binary_accuracy: 0.9258\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2727 - binary_accuracy: 0.9258\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2793 - binary_accuracy: 0.9258\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2759 - binary_accuracy: 0.9238\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2757 - binary_accuracy: 0.9199\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2829 - binary_accuracy: 0.9199\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2709 - binary_accuracy: 0.9180\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2789 - binary_accuracy: 0.9141\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2837 - binary_accuracy: 0.9316\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3626 - binary_accuracy: 0.8789\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3148 - binary_accuracy: 0.9141\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3302 - binary_accuracy: 0.9062\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2832 - binary_accuracy: 0.9219\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3012 - binary_accuracy: 0.9082\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2942 - binary_accuracy: 0.9199\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2771 - binary_accuracy: 0.9180\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3199 - binary_accuracy: 0.9023\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2819 - binary_accuracy: 0.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 4ms/step - loss: 0.8498 - binary_accuracy: 0.6621\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5696 - binary_accuracy: 0.7520\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5326 - binary_accuracy: 0.8066\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5119 - binary_accuracy: 0.8203\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4775 - binary_accuracy: 0.8242\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4416 - binary_accuracy: 0.8477\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4034 - binary_accuracy: 0.8496\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4380 - binary_accuracy: 0.8340\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5261 - binary_accuracy: 0.8594\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5040 - binary_accuracy: 0.8242\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3897 - binary_accuracy: 0.8535\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4758 - binary_accuracy: 0.8730\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3506 - binary_accuracy: 0.8652\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3675 - binary_accuracy: 0.8789\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3671 - binary_accuracy: 0.8633\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3390 - binary_accuracy: 0.8594\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4388 - binary_accuracy: 0.8535\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3676 - binary_accuracy: 0.8633\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3755 - binary_accuracy: 0.8652\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4423 - binary_accuracy: 0.8535\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3102 - binary_accuracy: 0.8887\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4360 - binary_accuracy: 0.8633\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3266 - binary_accuracy: 0.8789\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3380 - binary_accuracy: 0.8691\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2529 - binary_accuracy: 0.9062\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3698 - binary_accuracy: 0.8691\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3263 - binary_accuracy: 0.8887\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2979 - binary_accuracy: 0.8906\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3714 - binary_accuracy: 0.8828\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2743 - binary_accuracy: 0.8984\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2673 - binary_accuracy: 0.9082\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2833 - binary_accuracy: 0.9023\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3574 - binary_accuracy: 0.8848\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2625 - binary_accuracy: 0.9180\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2870 - binary_accuracy: 0.9102\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2945 - binary_accuracy: 0.8984\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2942 - binary_accuracy: 0.8984\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2990 - binary_accuracy: 0.9062\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2657 - binary_accuracy: 0.9121\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3400 - binary_accuracy: 0.9102\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3083 - binary_accuracy: 0.8906\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3349 - binary_accuracy: 0.8926\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2741 - binary_accuracy: 0.9199\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3164 - binary_accuracy: 0.8945\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3048 - binary_accuracy: 0.9160\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3173 - binary_accuracy: 0.9141\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3227 - binary_accuracy: 0.8926\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3169 - binary_accuracy: 0.9102\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3997 - binary_accuracy: 0.8789\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3691 - binary_accuracy: 0.8848\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2865 - binary_accuracy: 0.9062\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3160 - binary_accuracy: 0.9141\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2878 - binary_accuracy: 0.9004\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2853 - binary_accuracy: 0.9121\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2934 - binary_accuracy: 0.9043A: 0s - loss: 0.3235 - binary_accuracy: 0.\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3015 - binary_accuracy: 0.8984\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2837 - binary_accuracy: 0.9082\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3203 - binary_accuracy: 0.8945A: 0s - loss: 0.2539 - binary_accuracy: 0.\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2675 - binary_accuracy: 0.9141\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3304 - binary_accuracy: 0.8926\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3252 - binary_accuracy: 0.8867\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2635 - binary_accuracy: 0.9121\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2977 - binary_accuracy: 0.9082\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2625 - binary_accuracy: 0.9160\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3503 - binary_accuracy: 0.8926\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2812 - binary_accuracy: 0.9277\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2807 - binary_accuracy: 0.9121\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3032 - binary_accuracy: 0.9043\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3160 - binary_accuracy: 0.9004\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3136 - binary_accuracy: 0.9023\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2716 - binary_accuracy: 0.9160\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2828 - binary_accuracy: 0.8965\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4328 - binary_accuracy: 0.8770\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.3058 - binary_accuracy: 0.8887\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3574 - binary_accuracy: 0.8848\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3491 - binary_accuracy: 0.8945\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2727 - binary_accuracy: 0.9043\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3066 - binary_accuracy: 0.9141\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2688 - binary_accuracy: 0.9102\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2766 - binary_accuracy: 0.9023\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2945 - binary_accuracy: 0.8906\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2991 - binary_accuracy: 0.8906\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2665 - binary_accuracy: 0.9082\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3276 - binary_accuracy: 0.8906\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3625 - binary_accuracy: 0.8691\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2589 - binary_accuracy: 0.9121\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2971 - binary_accuracy: 0.9102\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2473 - binary_accuracy: 0.9141\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2797 - binary_accuracy: 0.9180\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2481 - binary_accuracy: 0.9238\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3206 - binary_accuracy: 0.9062\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2734 - binary_accuracy: 0.9023\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2437 - binary_accuracy: 0.9180\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3277 - binary_accuracy: 0.9043\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2900 - binary_accuracy: 0.8984\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2991 - binary_accuracy: 0.9023\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2583 - binary_accuracy: 0.9102\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2727 - binary_accuracy: 0.9043\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2884 - binary_accuracy: 0.9043\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2556 - binary_accuracy: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 5ms/step - loss: 0.8536 - binary_accuracy: 0.6152\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5251 - binary_accuracy: 0.6836\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4787 - binary_accuracy: 0.7168\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4322 - binary_accuracy: 0.8066\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3955 - binary_accuracy: 0.8438\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4277 - binary_accuracy: 0.8359\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3518 - binary_accuracy: 0.8730\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3665 - binary_accuracy: 0.8535\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3338 - binary_accuracy: 0.8613\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3787 - binary_accuracy: 0.8379\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3624 - binary_accuracy: 0.8633\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3327 - binary_accuracy: 0.8789\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3974 - binary_accuracy: 0.8438\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3189 - binary_accuracy: 0.8828\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3391 - binary_accuracy: 0.8809\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3101 - binary_accuracy: 0.8848\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3294 - binary_accuracy: 0.8809\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3031 - binary_accuracy: 0.8945\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3027 - binary_accuracy: 0.8867\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.3160 - binary_accuracy: 0.866 - 0s 5ms/step - loss: 0.3046 - binary_accuracy: 0.8730\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2995 - binary_accuracy: 0.8867\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2829 - binary_accuracy: 0.8906\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2778 - binary_accuracy: 0.8926\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2894 - binary_accuracy: 0.8926\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3023 - binary_accuracy: 0.8965\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2776 - binary_accuracy: 0.8887\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2922 - binary_accuracy: 0.9004\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2614 - binary_accuracy: 0.9082\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2730 - binary_accuracy: 0.9023\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3367 - binary_accuracy: 0.8887\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2906 - binary_accuracy: 0.8906\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2637 - binary_accuracy: 0.8984\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2743 - binary_accuracy: 0.9160\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2786 - binary_accuracy: 0.8984\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2744 - binary_accuracy: 0.9102\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2970 - binary_accuracy: 0.8906\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3033 - binary_accuracy: 0.8828\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3005 - binary_accuracy: 0.9062\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3344 - binary_accuracy: 0.8906\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3149 - binary_accuracy: 0.9004\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2706 - binary_accuracy: 0.8965\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2936 - binary_accuracy: 0.9023\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2825 - binary_accuracy: 0.9258\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2912 - binary_accuracy: 0.9043\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3730 - binary_accuracy: 0.8906\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2828 - binary_accuracy: 0.9180\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3293 - binary_accuracy: 0.9043\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3217 - binary_accuracy: 0.9082\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2609 - binary_accuracy: 0.9062\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2792 - binary_accuracy: 0.9180\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2840 - binary_accuracy: 0.9102\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2911 - binary_accuracy: 0.8945\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3180 - binary_accuracy: 0.9082\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2890 - binary_accuracy: 0.9160\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2974 - binary_accuracy: 0.8965\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2762 - binary_accuracy: 0.9043\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2732 - binary_accuracy: 0.9043\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3257 - binary_accuracy: 0.8984\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3375 - binary_accuracy: 0.9082\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3125 - binary_accuracy: 0.9062\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3124 - binary_accuracy: 0.9102\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2852 - binary_accuracy: 0.9160\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3248 - binary_accuracy: 0.8945\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2954 - binary_accuracy: 0.9043\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2862 - binary_accuracy: 0.9004\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3081 - binary_accuracy: 0.9121\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3003 - binary_accuracy: 0.9102\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3049 - binary_accuracy: 0.9062A: 0s - loss: 0.2935 - binary_accuracy: 0.9\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3012 - binary_accuracy: 0.9082\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2594 - binary_accuracy: 0.9121\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2703 - binary_accuracy: 0.9121\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4349 - binary_accuracy: 0.8848\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3141 - binary_accuracy: 0.8965\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2646 - binary_accuracy: 0.9238\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2605 - binary_accuracy: 0.9043\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2745 - binary_accuracy: 0.9160\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3147 - binary_accuracy: 0.8945\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2760 - binary_accuracy: 0.9121\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2792 - binary_accuracy: 0.9199\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2842 - binary_accuracy: 0.9180\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2636 - binary_accuracy: 0.9180\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3023 - binary_accuracy: 0.9004\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2880 - binary_accuracy: 0.9219\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2782 - binary_accuracy: 0.9180\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2957 - binary_accuracy: 0.8984\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2895 - binary_accuracy: 0.9160\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3257 - binary_accuracy: 0.9023\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3037 - binary_accuracy: 0.9082\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3074 - binary_accuracy: 0.9102\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3240 - binary_accuracy: 0.8965\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3240 - binary_accuracy: 0.9062\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3090 - binary_accuracy: 0.8926\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2957 - binary_accuracy: 0.9082\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3202 - binary_accuracy: 0.9102\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3082 - binary_accuracy: 0.9238\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2897 - binary_accuracy: 0.9004\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4333 - binary_accuracy: 0.9023A: 0s - loss: 0.2679 - binary_accuracy: 0.\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3099 - binary_accuracy: 0.9199\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3207 - binary_accuracy: 0.9102\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3131 - binary_accuracy: 0.9082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 5ms/step - loss: 0.7645 - binary_accuracy: 0.6582\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.6380 - binary_accuracy: 0.6816\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4811 - binary_accuracy: 0.7598\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5266 - binary_accuracy: 0.7539\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4460 - binary_accuracy: 0.8184\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4032 - binary_accuracy: 0.8379\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4040 - binary_accuracy: 0.8223\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.4003 - binary_accuracy: 0.831 - 0s 5ms/step - loss: 0.4107 - binary_accuracy: 0.8281\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4711 - binary_accuracy: 0.8105\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3914 - binary_accuracy: 0.8574\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3497 - binary_accuracy: 0.8770\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4313 - binary_accuracy: 0.8223\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4303 - binary_accuracy: 0.8418\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4054 - binary_accuracy: 0.8574\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4047 - binary_accuracy: 0.8340\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3735 - binary_accuracy: 0.8711\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3320 - binary_accuracy: 0.8652\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4747 - binary_accuracy: 0.8379\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3137 - binary_accuracy: 0.8750\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3341 - binary_accuracy: 0.8711\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3578 - binary_accuracy: 0.8613\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3090 - binary_accuracy: 0.8789\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2839 - binary_accuracy: 0.8945\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3216 - binary_accuracy: 0.8711\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3393 - binary_accuracy: 0.8652\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2816 - binary_accuracy: 0.9043\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3384 - binary_accuracy: 0.8770\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2965 - binary_accuracy: 0.8848\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3353 - binary_accuracy: 0.8789\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3254 - binary_accuracy: 0.8867\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3067 - binary_accuracy: 0.8789\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2962 - binary_accuracy: 0.8926\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2810 - binary_accuracy: 0.8984\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2873 - binary_accuracy: 0.8984\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3019 - binary_accuracy: 0.8926\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3222 - binary_accuracy: 0.8887\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3086 - binary_accuracy: 0.9004\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3156 - binary_accuracy: 0.9023\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3195 - binary_accuracy: 0.8789\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3117 - binary_accuracy: 0.8945\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3077 - binary_accuracy: 0.8965\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3097 - binary_accuracy: 0.9023\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2898 - binary_accuracy: 0.9082\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3068 - binary_accuracy: 0.8926\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3165 - binary_accuracy: 0.9004\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2933 - binary_accuracy: 0.8984\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3064 - binary_accuracy: 0.8965\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3103 - binary_accuracy: 0.8848\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2622 - binary_accuracy: 0.9023\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2757 - binary_accuracy: 0.9023\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2859 - binary_accuracy: 0.8984\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2664 - binary_accuracy: 0.9102\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.2807 - binary_accuracy: 0.906 - 0s 6ms/step - loss: 0.2745 - binary_accuracy: 0.9082\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3404 - binary_accuracy: 0.8809\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2851 - binary_accuracy: 0.8965\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2928 - binary_accuracy: 0.9023\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2969 - binary_accuracy: 0.8945\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2613 - binary_accuracy: 0.9004\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2886 - binary_accuracy: 0.9043\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2598 - binary_accuracy: 0.9082\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2567 - binary_accuracy: 0.9141\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2802 - binary_accuracy: 0.9141\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3024 - binary_accuracy: 0.9121\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3226 - binary_accuracy: 0.8887\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3170 - binary_accuracy: 0.9082\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3041 - binary_accuracy: 0.8789\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2815 - binary_accuracy: 0.9004\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2853 - binary_accuracy: 0.9102\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3290 - binary_accuracy: 0.8887\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3043 - binary_accuracy: 0.9023\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2936 - binary_accuracy: 0.9062\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.2737 - binary_accuracy: 0.9102\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2997 - binary_accuracy: 0.9043\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.2574 - binary_accuracy: 0.9102\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3000 - binary_accuracy: 0.9043\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2787 - binary_accuracy: 0.9102\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2946 - binary_accuracy: 0.8906\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2709 - binary_accuracy: 0.9082\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3359 - binary_accuracy: 0.8691\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2870 - binary_accuracy: 0.9043A: 0s - loss: 0.2913 - binary_accuracy: 0.\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3516 - binary_accuracy: 0.8770\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3050 - binary_accuracy: 0.9004\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3095 - binary_accuracy: 0.8965\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2655 - binary_accuracy: 0.8984\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2688 - binary_accuracy: 0.9180\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2543 - binary_accuracy: 0.9121\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3009 - binary_accuracy: 0.8984\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2919 - binary_accuracy: 0.8926\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2785 - binary_accuracy: 0.9023\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2607 - binary_accuracy: 0.9043\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2669 - binary_accuracy: 0.9141\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2503 - binary_accuracy: 0.9082\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2694 - binary_accuracy: 0.9082\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2420 - binary_accuracy: 0.9199\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3083 - binary_accuracy: 0.8945\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2683 - binary_accuracy: 0.9160\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.2578 - binary_accuracy: 0.898 - 0s 6ms/step - loss: 0.2499 - binary_accuracy: 0.9023\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2572 - binary_accuracy: 0.9180\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2685 - binary_accuracy: 0.9023\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2795 - binary_accuracy: 0.9082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 6ms/step - loss: 0.6787 - binary_accuracy: 0.6406\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6400 - binary_accuracy: 0.6797\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5207 - binary_accuracy: 0.7539\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5112 - binary_accuracy: 0.7539\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4422 - binary_accuracy: 0.8203\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4474 - binary_accuracy: 0.8320\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4059 - binary_accuracy: 0.8301\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5364 - binary_accuracy: 0.8105\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3495 - binary_accuracy: 0.8574\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4544 - binary_accuracy: 0.8145\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4032 - binary_accuracy: 0.8398\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3739 - binary_accuracy: 0.8516\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3585 - binary_accuracy: 0.8418\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3371 - binary_accuracy: 0.8613\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3505 - binary_accuracy: 0.8496\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4984 - binary_accuracy: 0.8496\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4790 - binary_accuracy: 0.8652\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4260 - binary_accuracy: 0.8496\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3871 - binary_accuracy: 0.8359\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3843 - binary_accuracy: 0.8613\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3764 - binary_accuracy: 0.8750A: 0s - loss: 0.4488 - binary_accuracy: 0.\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3877 - binary_accuracy: 0.8555\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3642 - binary_accuracy: 0.8574\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4335 - binary_accuracy: 0.8125\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4559 - binary_accuracy: 0.8145\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3311 - binary_accuracy: 0.8730\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3138 - binary_accuracy: 0.8789\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3547 - binary_accuracy: 0.8691\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3404 - binary_accuracy: 0.8848\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3147 - binary_accuracy: 0.8730\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3558 - binary_accuracy: 0.8555\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3199 - binary_accuracy: 0.8926\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3451 - binary_accuracy: 0.8750\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2907 - binary_accuracy: 0.8984\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3472 - binary_accuracy: 0.8555\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2941 - binary_accuracy: 0.8887\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2927 - binary_accuracy: 0.8789\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2831 - binary_accuracy: 0.9004\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3021 - binary_accuracy: 0.8848\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2774 - binary_accuracy: 0.8828\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2893 - binary_accuracy: 0.8887\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2811 - binary_accuracy: 0.8887\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2842 - binary_accuracy: 0.8965\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2687 - binary_accuracy: 0.8984\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2746 - binary_accuracy: 0.8926\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2869 - binary_accuracy: 0.8945\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2684 - binary_accuracy: 0.9062\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2980 - binary_accuracy: 0.9043\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2898 - binary_accuracy: 0.8887\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3088 - binary_accuracy: 0.8789\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2689 - binary_accuracy: 0.8945\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2793 - binary_accuracy: 0.8945\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2718 - binary_accuracy: 0.9082\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2708 - binary_accuracy: 0.9082\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2839 - binary_accuracy: 0.9043\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3136 - binary_accuracy: 0.8906\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2938 - binary_accuracy: 0.8926\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2973 - binary_accuracy: 0.8984\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2675 - binary_accuracy: 0.8926\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2724 - binary_accuracy: 0.9023\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2724 - binary_accuracy: 0.9062\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2948 - binary_accuracy: 0.8848\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.2886 - binary_accuracy: 0.8848\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2561 - binary_accuracy: 0.8926\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2512 - binary_accuracy: 0.9102\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2960 - binary_accuracy: 0.9004\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2917 - binary_accuracy: 0.9102\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3077 - binary_accuracy: 0.8965\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2496 - binary_accuracy: 0.8965\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2906 - binary_accuracy: 0.8984\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2805 - binary_accuracy: 0.9004\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2821 - binary_accuracy: 0.8809\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2537 - binary_accuracy: 0.9023\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2806 - binary_accuracy: 0.9121\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2622 - binary_accuracy: 0.9121\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2889 - binary_accuracy: 0.9023\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2752 - binary_accuracy: 0.9023\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2870 - binary_accuracy: 0.9082\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.2787 - binary_accuracy: 0.908 - 0s 5ms/step - loss: 0.2777 - binary_accuracy: 0.9082\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2735 - binary_accuracy: 0.9062\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2848 - binary_accuracy: 0.8945\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3081 - binary_accuracy: 0.8926\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2754 - binary_accuracy: 0.9102\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2661 - binary_accuracy: 0.9082\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2814 - binary_accuracy: 0.8945\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2765 - binary_accuracy: 0.9004\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2755 - binary_accuracy: 0.8906\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2799 - binary_accuracy: 0.8867\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2776 - binary_accuracy: 0.9043\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3088 - binary_accuracy: 0.8926\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3004 - binary_accuracy: 0.8906\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2984 - binary_accuracy: 0.8945\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3207 - binary_accuracy: 0.8926\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3066 - binary_accuracy: 0.8887\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3246 - binary_accuracy: 0.8809\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2858 - binary_accuracy: 0.9004\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2725 - binary_accuracy: 0.8848\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3169 - binary_accuracy: 0.8906\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2691 - binary_accuracy: 0.9004\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2757 - binary_accuracy: 0.9023\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 4ms/step - loss: 0.7540 - binary_accuracy: 0.6426\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.5364 - binary_accuracy: 0.7266\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4632 - binary_accuracy: 0.7480\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4316 - binary_accuracy: 0.7734\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4651 - binary_accuracy: 0.7656\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3999 - binary_accuracy: 0.8125\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4311 - binary_accuracy: 0.8164\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4630 - binary_accuracy: 0.8008\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3923 - binary_accuracy: 0.8457\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3515 - binary_accuracy: 0.8750\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.3077 - binary_accuracy: 0.887 - 0s 5ms/step - loss: 0.2997 - binary_accuracy: 0.8906\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2956 - binary_accuracy: 0.8906\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3591 - binary_accuracy: 0.8848\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3094 - binary_accuracy: 0.8711\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3291 - binary_accuracy: 0.8594\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3402 - binary_accuracy: 0.8828\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2851 - binary_accuracy: 0.9082\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3497 - binary_accuracy: 0.8574\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2971 - binary_accuracy: 0.9062\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3020 - binary_accuracy: 0.8984\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2766 - binary_accuracy: 0.8906\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3246 - binary_accuracy: 0.8828\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2811 - binary_accuracy: 0.9062\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2892 - binary_accuracy: 0.8965\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2590 - binary_accuracy: 0.9062\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2682 - binary_accuracy: 0.9023\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2963 - binary_accuracy: 0.9043\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2658 - binary_accuracy: 0.8945\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2544 - binary_accuracy: 0.9043\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2649 - binary_accuracy: 0.9121\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2739 - binary_accuracy: 0.9023\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2434 - binary_accuracy: 0.9062\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3066 - binary_accuracy: 0.9004\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3390 - binary_accuracy: 0.8770\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2939 - binary_accuracy: 0.9023\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2480 - binary_accuracy: 0.9141\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2283 - binary_accuracy: 0.9180\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2909 - binary_accuracy: 0.9043\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2585 - binary_accuracy: 0.9160\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2506 - binary_accuracy: 0.9082A: 0s - loss: 0.2567 - binary_accuracy: 0.\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2691 - binary_accuracy: 0.9199\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2979 - binary_accuracy: 0.9199\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2695 - binary_accuracy: 0.9238\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2459 - binary_accuracy: 0.9121\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2921 - binary_accuracy: 0.9102\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3416 - binary_accuracy: 0.9160\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2535 - binary_accuracy: 0.9082\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2552 - binary_accuracy: 0.9180\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2536 - binary_accuracy: 0.9102\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3014 - binary_accuracy: 0.9023\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2695 - binary_accuracy: 0.9102\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2738 - binary_accuracy: 0.9121\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2522 - binary_accuracy: 0.8984\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2524 - binary_accuracy: 0.9043\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2494 - binary_accuracy: 0.9199\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2615 - binary_accuracy: 0.9121\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2486 - binary_accuracy: 0.9219\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2729 - binary_accuracy: 0.9160\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2702 - binary_accuracy: 0.9121\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2948 - binary_accuracy: 0.9043\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2986 - binary_accuracy: 0.9121\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2741 - binary_accuracy: 0.9121\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.2502 - binary_accuracy: 0.9141\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2509 - binary_accuracy: 0.9160\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2649 - binary_accuracy: 0.9180\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2802 - binary_accuracy: 0.9141\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2541 - binary_accuracy: 0.9199\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2227 - binary_accuracy: 0.9277\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2729 - binary_accuracy: 0.9219\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2582 - binary_accuracy: 0.9121\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2511 - binary_accuracy: 0.9219\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2351 - binary_accuracy: 0.9238\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2588 - binary_accuracy: 0.9141\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2348 - binary_accuracy: 0.9258\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2408 - binary_accuracy: 0.9199\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2568 - binary_accuracy: 0.9004\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2510 - binary_accuracy: 0.9180\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2608 - binary_accuracy: 0.9102\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2587 - binary_accuracy: 0.9258\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2445 - binary_accuracy: 0.9297\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2494 - binary_accuracy: 0.9297\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2469 - binary_accuracy: 0.9121\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2630 - binary_accuracy: 0.9160\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2348 - binary_accuracy: 0.9238\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2513 - binary_accuracy: 0.9258\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2216 - binary_accuracy: 0.9219\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2787 - binary_accuracy: 0.9141\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2370 - binary_accuracy: 0.9258\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2266 - binary_accuracy: 0.9277\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2647 - binary_accuracy: 0.9121\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2463 - binary_accuracy: 0.9180\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2242 - binary_accuracy: 0.9160\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2165 - binary_accuracy: 0.9199\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2323 - binary_accuracy: 0.9316\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2649 - binary_accuracy: 0.9199\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2632 - binary_accuracy: 0.9062\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2382 - binary_accuracy: 0.9238\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2358 - binary_accuracy: 0.9160\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2474 - binary_accuracy: 0.9238\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2349 - binary_accuracy: 0.9160\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 8ms/step - loss: 0.9159 - binary_accuracy: 0.6491\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5096 - binary_accuracy: 0.7310\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4836 - binary_accuracy: 0.7817\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5385 - binary_accuracy: 0.7583\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5226 - binary_accuracy: 0.7719\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4584 - binary_accuracy: 0.8070\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4903 - binary_accuracy: 0.8070\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6757 - binary_accuracy: 0.7895\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7576 - binary_accuracy: 0.8031\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7954 - binary_accuracy: 0.8187\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6756 - binary_accuracy: 0.8168\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4351 - binary_accuracy: 0.8655\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5757 - binary_accuracy: 0.8226\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5151 - binary_accuracy: 0.8460A: 0s - loss: 0.5006 - binary_accuracy: 0.85\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4714 - binary_accuracy: 0.8148\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4089 - binary_accuracy: 0.8538\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4028 - binary_accuracy: 0.8694\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4017 - binary_accuracy: 0.8519\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4080 - binary_accuracy: 0.8421\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4175 - binary_accuracy: 0.8655\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3742 - binary_accuracy: 0.8519\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3718 - binary_accuracy: 0.8538\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4132 - binary_accuracy: 0.8519\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3596 - binary_accuracy: 0.8811\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3512 - binary_accuracy: 0.8889\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3372 - binary_accuracy: 0.8811\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3189 - binary_accuracy: 0.8947\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3662 - binary_accuracy: 0.8558\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3299 - binary_accuracy: 0.8869\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3271 - binary_accuracy: 0.8830\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3261 - binary_accuracy: 0.8830\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3840 - binary_accuracy: 0.8674\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3901 - binary_accuracy: 0.8480\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3862 - binary_accuracy: 0.8869\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3009 - binary_accuracy: 0.8811\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3372 - binary_accuracy: 0.8889\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3145 - binary_accuracy: 0.8889\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2829 - binary_accuracy: 0.9084\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3219 - binary_accuracy: 0.8772\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3347 - binary_accuracy: 0.8889\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2872 - binary_accuracy: 0.8947\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3238 - binary_accuracy: 0.8830\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3313 - binary_accuracy: 0.8947\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2957 - binary_accuracy: 0.8928\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2911 - binary_accuracy: 0.8908\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2726 - binary_accuracy: 0.8967\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2701 - binary_accuracy: 0.9006\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2700 - binary_accuracy: 0.9162\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2807 - binary_accuracy: 0.8986\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2697 - binary_accuracy: 0.8947\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2944 - binary_accuracy: 0.8811\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2822 - binary_accuracy: 0.8947\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2918 - binary_accuracy: 0.9045\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3145 - binary_accuracy: 0.8830\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2922 - binary_accuracy: 0.8928\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2614 - binary_accuracy: 0.9064\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2422 - binary_accuracy: 0.9025\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2447 - binary_accuracy: 0.9025\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2841 - binary_accuracy: 0.9006\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2725 - binary_accuracy: 0.9103\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2771 - binary_accuracy: 0.8947\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2612 - binary_accuracy: 0.8986\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2637 - binary_accuracy: 0.9025\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2776 - binary_accuracy: 0.9025\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.2677 - binary_accuracy: 0.8967\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2756 - binary_accuracy: 0.9006\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3090 - binary_accuracy: 0.8850\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2716 - binary_accuracy: 0.8947\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2548 - binary_accuracy: 0.8986\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2537 - binary_accuracy: 0.9201\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2626 - binary_accuracy: 0.9103\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2454 - binary_accuracy: 0.9045\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2602 - binary_accuracy: 0.9045\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2554 - binary_accuracy: 0.9025\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3323 - binary_accuracy: 0.8830\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2881 - binary_accuracy: 0.9045\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2844 - binary_accuracy: 0.8947\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2664 - binary_accuracy: 0.9006\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.2721 - binary_accuracy: 0.8986\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.2587 - binary_accuracy: 0.9064\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.2704 - binary_accuracy: 0.9006\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2726 - binary_accuracy: 0.9025\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2567 - binary_accuracy: 0.9162\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2560 - binary_accuracy: 0.9103\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2776 - binary_accuracy: 0.9123\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2723 - binary_accuracy: 0.9006\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2737 - binary_accuracy: 0.8986\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2657 - binary_accuracy: 0.9084\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2779 - binary_accuracy: 0.9123\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2765 - binary_accuracy: 0.9123\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2931 - binary_accuracy: 0.9103\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2744 - binary_accuracy: 0.9025\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2625 - binary_accuracy: 0.8986\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2805 - binary_accuracy: 0.9142\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2794 - binary_accuracy: 0.8986\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2830 - binary_accuracy: 0.8967\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2571 - binary_accuracy: 0.9064\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2611 - binary_accuracy: 0.9142\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2934 - binary_accuracy: 0.9045\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2632 - binary_accuracy: 0.9103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "resultados = cross_val_score(\n",
    "    estimator= classificador,\n",
    "    X = previsores,\n",
    "    y = classes,\n",
    "    cv = 10,\n",
    "    scoring = 'accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75438596, 0.84210526, 0.96491228, 0.8245614 , 0.80701754,\n",
       "       0.85964912, 0.84210526, 0.85964912, 0.84210526, 0.96428571])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856077694235589"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06156827288305725"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting e Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede_DropOut():\n",
    "    classificador3 = Sequential()\n",
    "    # Criando camada oculta\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = qt_neuronios,                  # Número de Neurônios da camada oculta\n",
    "            activation = 'relu',                   # Função de Ativação\n",
    "            kernel_initializer = 'random_uniform', # Kernel Inicial (random)\n",
    "            input_dim = qt_entradas                # Número de neurônios da camada de entrada\n",
    "        )\n",
    "    )\n",
    "\n",
    "    classificador3.add( \n",
    "        Dropout(0.2)\n",
    "    )\n",
    "\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = qt_neuronios,                  # Número de Neurônios da camada oculta\n",
    "            activation = 'relu',                   # Função de Ativação\n",
    "            kernel_initializer = 'random_uniform', # Kernel (random)\n",
    "        )\n",
    "    )\n",
    "    classificador3.add( \n",
    "        Dropout(0.2)\n",
    "    )\n",
    "        \n",
    "    # Criando camada de saída\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = 1,                             # Número de Neurônios da saida\n",
    "            activation = 'sigmoid',                # Função de Ativação (sigmoide: ou 0 ou 1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    otimizador = tf.keras.optimizers.Adam(\n",
    "        lr =  0.001,    # Taxa de Aprendizagem\n",
    "        decay = 0.001,  # Decaimento\n",
    "        clipvalue = 0.5 # Evitar que fique preso\n",
    "    )\n",
    "\n",
    "    classificador3.compile(\n",
    "        optimizer= otimizador,           # Otimização da função de erro (Adam personalizado)\n",
    "        loss = 'binary_crossentropy',    # Maneira como calculamos o erro (neste caso entropia para binário)\n",
    "        metrics = ['binary_accuracy']    # Métricas \n",
    "    )  \n",
    "    \n",
    "    return classificador3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_DropOut = KerasClassifier(\n",
    "    build_fn= criarRede_DropOut,\n",
    "    epochs = 100,\n",
    "    batch_size = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 2s 7ms/step - loss: 1.5176 - binary_accuracy: 0.5527\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6250 - binary_accuracy: 0.6641\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5562 - binary_accuracy: 0.7207\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6796 - binary_accuracy: 0.6992\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5933 - binary_accuracy: 0.7168\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5344 - binary_accuracy: 0.7285\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5355 - binary_accuracy: 0.7441\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4784 - binary_accuracy: 0.7871\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5607 - binary_accuracy: 0.7793\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5274 - binary_accuracy: 0.7930A: 0s - loss: 0.4807 - binary_accuracy: 0.\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5170 - binary_accuracy: 0.7637\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5426 - binary_accuracy: 0.7734\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5374 - binary_accuracy: 0.7773\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5140 - binary_accuracy: 0.7715\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5490 - binary_accuracy: 0.7656\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5702 - binary_accuracy: 0.7656\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5321 - binary_accuracy: 0.7812\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4647 - binary_accuracy: 0.7715\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5457 - binary_accuracy: 0.7617\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5365 - binary_accuracy: 0.7754\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6015 - binary_accuracy: 0.7559\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5144 - binary_accuracy: 0.7852\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5661 - binary_accuracy: 0.7578\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5460 - binary_accuracy: 0.7891\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5745 - binary_accuracy: 0.7734\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5593 - binary_accuracy: 0.7695\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5519 - binary_accuracy: 0.7871\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.5040 - binary_accuracy: 0.795 - 1s 13ms/step - loss: 0.5107 - binary_accuracy: 0.7949\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.5022 - binary_accuracy: 0.7754A: 0s - loss: 0.5399 - binary_accuracy: 0.7\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4786 - binary_accuracy: 0.7930\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4880 - binary_accuracy: 0.8008\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4991 - binary_accuracy: 0.7910\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5024 - binary_accuracy: 0.7930\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4890 - binary_accuracy: 0.7793\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5100 - binary_accuracy: 0.7832\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5099 - binary_accuracy: 0.7910\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4653 - binary_accuracy: 0.8047\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.4576 - binary_accuracy: 0.8164\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.4586 - binary_accuracy: 0.8027\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.4787 - binary_accuracy: 0.8008\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.4901 - binary_accuracy: 0.8164\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4733 - binary_accuracy: 0.8105\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4722 - binary_accuracy: 0.7910\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4353 - binary_accuracy: 0.8281\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4600 - binary_accuracy: 0.8066\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4535 - binary_accuracy: 0.8105\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4559 - binary_accuracy: 0.8145\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.4180 - binary_accuracy: 0.8223\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.5074 - binary_accuracy: 0.8105\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4692 - binary_accuracy: 0.8262\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.4296 - binary_accuracy: 0.8145\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4272 - binary_accuracy: 0.8320\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.4689 - binary_accuracy: 0.8047\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4552 - binary_accuracy: 0.8359\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4208 - binary_accuracy: 0.8125\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.4323 - binary_accuracy: 0.8262\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4282 - binary_accuracy: 0.8281\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4221 - binary_accuracy: 0.8164\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4304 - binary_accuracy: 0.8203\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4054 - binary_accuracy: 0.8516\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4162 - binary_accuracy: 0.8223\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4477 - binary_accuracy: 0.8262\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4508 - binary_accuracy: 0.8340\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5075 - binary_accuracy: 0.8027\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4450 - binary_accuracy: 0.8184\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4223 - binary_accuracy: 0.8242\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4659 - binary_accuracy: 0.8086\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4199 - binary_accuracy: 0.8301\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4288 - binary_accuracy: 0.8398\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4830 - binary_accuracy: 0.8262\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4090 - binary_accuracy: 0.8320\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4174 - binary_accuracy: 0.8262\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4296 - binary_accuracy: 0.8242\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4060 - binary_accuracy: 0.8457\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4215 - binary_accuracy: 0.8418\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4469 - binary_accuracy: 0.8203\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4416 - binary_accuracy: 0.8027\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4516 - binary_accuracy: 0.8301\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4044 - binary_accuracy: 0.8418\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4174 - binary_accuracy: 0.8438\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4237 - binary_accuracy: 0.8379\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3812 - binary_accuracy: 0.8535\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.3937 - binary_accuracy: 0.8633\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4061 - binary_accuracy: 0.8340\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4262 - binary_accuracy: 0.8320\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4414 - binary_accuracy: 0.8145\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4159 - binary_accuracy: 0.8320\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4281 - binary_accuracy: 0.8262\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4140 - binary_accuracy: 0.8320\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4123 - binary_accuracy: 0.8418\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3819 - binary_accuracy: 0.8379\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3897 - binary_accuracy: 0.8457\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4142 - binary_accuracy: 0.8379\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4326 - binary_accuracy: 0.8457\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4014 - binary_accuracy: 0.8535\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.3761 - binary_accuracy: 0.8691\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3863 - binary_accuracy: 0.8516\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4368 - binary_accuracy: 0.8379\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3963 - binary_accuracy: 0.8379\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4117 - binary_accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 6ms/step - loss: 1.0181 - binary_accuracy: 0.6211\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6766 - binary_accuracy: 0.6328\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7238 - binary_accuracy: 0.6523\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6608 - binary_accuracy: 0.6992\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6012 - binary_accuracy: 0.6660\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7054 - binary_accuracy: 0.7109\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6107 - binary_accuracy: 0.7031\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6767 - binary_accuracy: 0.7539\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.7314 - binary_accuracy: 0.7676\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.6891 - binary_accuracy: 0.7676\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.7154 - binary_accuracy: 0.7246\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.7284 - binary_accuracy: 0.7793\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5549 - binary_accuracy: 0.7656\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8322 - binary_accuracy: 0.7910\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5138 - binary_accuracy: 0.7715\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7000 - binary_accuracy: 0.7559\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8980 - binary_accuracy: 0.8105\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5615 - binary_accuracy: 0.7832\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9019 - binary_accuracy: 0.8184\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7771 - binary_accuracy: 0.8047\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.5274 - binary_accuracy: 0.8027\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7493 - binary_accuracy: 0.7930A: 0s - loss: 0.6160 - binary_accuracy: 0.8\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.2538 - binary_accuracy: 0.7988A: 0s - loss: 0.4830 - binary_accuracy: 0\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7934 - binary_accuracy: 0.8008\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7400 - binary_accuracy: 0.8242\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7876 - binary_accuracy: 0.8340\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6272 - binary_accuracy: 0.8125\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9339 - binary_accuracy: 0.8320\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5942 - binary_accuracy: 0.7988\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.4172 - binary_accuracy: 0.8027A: 0s - loss: 1.2036 - binary_accuracy: 0.\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8687 - binary_accuracy: 0.8223\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9569 - binary_accuracy: 0.8320\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0758 - binary_accuracy: 0.8125\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9118 - binary_accuracy: 0.8047\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6214 - binary_accuracy: 0.8164\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8680 - binary_accuracy: 0.8281\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.4101 - binary_accuracy: 0.8223\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7380 - binary_accuracy: 0.8320\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5657 - binary_accuracy: 0.8242\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.8708 - binary_accuracy: 0.8105\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.9279 - binary_accuracy: 0.8105\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 1.2522 - binary_accuracy: 0.8223\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5669 - binary_accuracy: 0.8398\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7654 - binary_accuracy: 0.8223\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6602 - binary_accuracy: 0.8652\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.5029 - binary_accuracy: 0.8438\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5388 - binary_accuracy: 0.8242\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5852 - binary_accuracy: 0.8203\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.2320 - binary_accuracy: 0.8145\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6760 - binary_accuracy: 0.8535\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.0362 - binary_accuracy: 0.8398\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7871 - binary_accuracy: 0.8594\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7417 - binary_accuracy: 0.8379\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8450 - binary_accuracy: 0.8359\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9593 - binary_accuracy: 0.8398\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6785 - binary_accuracy: 0.8398\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9103 - binary_accuracy: 0.8281\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4458 - binary_accuracy: 0.8652\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5570 - binary_accuracy: 0.8555\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0861 - binary_accuracy: 0.8594\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4766 - binary_accuracy: 0.8242\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8025 - binary_accuracy: 0.8672\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.8621 - binary_accuracy: 0.8438\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5303 - binary_accuracy: 0.8457\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4103 - binary_accuracy: 0.8457\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6822 - binary_accuracy: 0.8574\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.5704 - binary_accuracy: 0.8340\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7058 - binary_accuracy: 0.8535\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6746 - binary_accuracy: 0.8496\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7209 - binary_accuracy: 0.8691\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5770 - binary_accuracy: 0.8594\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7651 - binary_accuracy: 0.8516\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7598 - binary_accuracy: 0.8594\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9090 - binary_accuracy: 0.8457\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4526 - binary_accuracy: 0.8672\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4744 - binary_accuracy: 0.8789\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.5970 - binary_accuracy: 0.8574\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.1706 - binary_accuracy: 0.8496\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5940 - binary_accuracy: 0.8633\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.0703 - binary_accuracy: 0.8672\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6402 - binary_accuracy: 0.8730\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9935 - binary_accuracy: 0.8867\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5978 - binary_accuracy: 0.8672\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.2992 - binary_accuracy: 0.8457\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6961 - binary_accuracy: 0.8711\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8881 - binary_accuracy: 0.8379\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6672 - binary_accuracy: 0.8438\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9111 - binary_accuracy: 0.8770\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 1.1577 - binary_accuracy: 0.8789\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.5779 - binary_accuracy: 0.8750\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6868 - binary_accuracy: 0.8711\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.2240 - binary_accuracy: 0.8555\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5783 - binary_accuracy: 0.8535\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7835 - binary_accuracy: 0.8887\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9340 - binary_accuracy: 0.8691\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0213 - binary_accuracy: 0.8789\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6850 - binary_accuracy: 0.8828\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4559 - binary_accuracy: 0.8809\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.3681 - binary_accuracy: 0.8535\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 1.1483 - binary_accuracy: 0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 6ms/step - loss: 1.0987 - binary_accuracy: 0.6328\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7237 - binary_accuracy: 0.6426\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6684 - binary_accuracy: 0.6973\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5689 - binary_accuracy: 0.7383\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6047 - binary_accuracy: 0.7383\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6274 - binary_accuracy: 0.7422\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6008 - binary_accuracy: 0.7500\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.5963 - binary_accuracy: 0.7871\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6184 - binary_accuracy: 0.7695\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5119 - binary_accuracy: 0.7891\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5334 - binary_accuracy: 0.7812\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5090 - binary_accuracy: 0.7832\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5540 - binary_accuracy: 0.8125\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5379 - binary_accuracy: 0.8047\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6179 - binary_accuracy: 0.7852\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4499 - binary_accuracy: 0.7949\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5885 - binary_accuracy: 0.7559\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5537 - binary_accuracy: 0.8086\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5221 - binary_accuracy: 0.7988\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4759 - binary_accuracy: 0.8047\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5547 - binary_accuracy: 0.7930\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6017 - binary_accuracy: 0.8047\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4691 - binary_accuracy: 0.7891\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5167 - binary_accuracy: 0.8086\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4503 - binary_accuracy: 0.8184\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5209 - binary_accuracy: 0.7988\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4423 - binary_accuracy: 0.8145\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4467 - binary_accuracy: 0.8340\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5931 - binary_accuracy: 0.8027\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4272 - binary_accuracy: 0.8242\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6488 - binary_accuracy: 0.8203\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4628 - binary_accuracy: 0.8066\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6107 - binary_accuracy: 0.8281\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6842 - binary_accuracy: 0.8281\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5016 - binary_accuracy: 0.8359\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5948 - binary_accuracy: 0.8301\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6994 - binary_accuracy: 0.8516\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8061 - binary_accuracy: 0.8184\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5479 - binary_accuracy: 0.8223\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7244 - binary_accuracy: 0.8242\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4397 - binary_accuracy: 0.8438\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.5894 - binary_accuracy: 0.8516\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4149 - binary_accuracy: 0.8340\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6625 - binary_accuracy: 0.8203\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4239 - binary_accuracy: 0.8418\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5949 - binary_accuracy: 0.8379\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3870 - binary_accuracy: 0.8438\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7938 - binary_accuracy: 0.8418\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4952 - binary_accuracy: 0.8457\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5899 - binary_accuracy: 0.8516\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4885 - binary_accuracy: 0.8145\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4957 - binary_accuracy: 0.8516\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6755 - binary_accuracy: 0.8555\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3794 - binary_accuracy: 0.8730\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4002 - binary_accuracy: 0.8594\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5581 - binary_accuracy: 0.8438\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7578 - binary_accuracy: 0.8398\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4007 - binary_accuracy: 0.8633\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5406 - binary_accuracy: 0.8438\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7895 - binary_accuracy: 0.8516\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.5797 - binary_accuracy: 0.8652\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7172 - binary_accuracy: 0.8555\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.1267 - binary_accuracy: 0.8516\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6163 - binary_accuracy: 0.8516\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5978 - binary_accuracy: 0.8613\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4072 - binary_accuracy: 0.8613\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9101 - binary_accuracy: 0.8574\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8324 - binary_accuracy: 0.8711\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4984 - binary_accuracy: 0.8594\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6865 - binary_accuracy: 0.8496\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9402 - binary_accuracy: 0.8633\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0115 - binary_accuracy: 0.8770\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5464 - binary_accuracy: 0.8496\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6070 - binary_accuracy: 0.8633\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8700 - binary_accuracy: 0.8672\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4568 - binary_accuracy: 0.8809\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9384 - binary_accuracy: 0.8691\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5058 - binary_accuracy: 0.8848\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.2615 - binary_accuracy: 0.8652\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.1378 - binary_accuracy: 0.8711\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6870 - binary_accuracy: 0.8496\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.8088 - binary_accuracy: 0.8594\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.9435 - binary_accuracy: 0.8535\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4957 - binary_accuracy: 0.8418\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4965 - binary_accuracy: 0.8633\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5704 - binary_accuracy: 0.8613\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4954 - binary_accuracy: 0.8711\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8069 - binary_accuracy: 0.8594\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4057 - binary_accuracy: 0.8691\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6717 - binary_accuracy: 0.8809\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3852 - binary_accuracy: 0.8672\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0584 - binary_accuracy: 0.8418\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0267 - binary_accuracy: 0.8691\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0757 - binary_accuracy: 0.8633\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8236 - binary_accuracy: 0.8750\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.2817 - binary_accuracy: 0.8652\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5861 - binary_accuracy: 0.8750\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9397 - binary_accuracy: 0.8691\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.0237 - binary_accuracy: 0.8809\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.1989 - binary_accuracy: 0.8594\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 6ms/step - loss: 1.4642 - binary_accuracy: 0.5469\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7263 - binary_accuracy: 0.6367\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6309 - binary_accuracy: 0.6582\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5798 - binary_accuracy: 0.6680\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5390 - binary_accuracy: 0.7324\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5963 - binary_accuracy: 0.7227\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6297 - binary_accuracy: 0.7109\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5705 - binary_accuracy: 0.7031\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5418 - binary_accuracy: 0.7559\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5559 - binary_accuracy: 0.7656\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5647 - binary_accuracy: 0.7637\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5621 - binary_accuracy: 0.7715\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.5755 - binary_accuracy: 0.7910: 0s - loss: 0.5762 - binary_accuracy: 0.790 - ETA: 0s - loss: 0.5755 - binary_accuracy: 0.791\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5074 - binary_accuracy: 0.8184\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5758 - binary_accuracy: 0.8027\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5454 - binary_accuracy: 0.8164\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5058 - binary_accuracy: 0.8008\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5287 - binary_accuracy: 0.8164\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4185 - binary_accuracy: 0.8164\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4574 - binary_accuracy: 0.7871\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5351 - binary_accuracy: 0.8027\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5182 - binary_accuracy: 0.8320\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4366 - binary_accuracy: 0.8086\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6262 - binary_accuracy: 0.8320\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5862 - binary_accuracy: 0.8262\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5368 - binary_accuracy: 0.8477\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6963 - binary_accuracy: 0.8320\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4300 - binary_accuracy: 0.8516\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6539 - binary_accuracy: 0.8281\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5423 - binary_accuracy: 0.8574\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4566 - binary_accuracy: 0.8633\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5190 - binary_accuracy: 0.8516\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7154 - binary_accuracy: 0.8613\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7040 - binary_accuracy: 0.8594\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5755 - binary_accuracy: 0.8496\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5313 - binary_accuracy: 0.8496\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4308 - binary_accuracy: 0.8691\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5446 - binary_accuracy: 0.8594\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7334 - binary_accuracy: 0.8652\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7117 - binary_accuracy: 0.8652\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4167 - binary_accuracy: 0.8809\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6917 - binary_accuracy: 0.8516\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3811 - binary_accuracy: 0.8535\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7939 - binary_accuracy: 0.8613\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.6177 - binary_accuracy: 0.8633\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7254 - binary_accuracy: 0.8789\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7765 - binary_accuracy: 0.8730\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8647 - binary_accuracy: 0.8574\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7022 - binary_accuracy: 0.8633\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4980 - binary_accuracy: 0.8711\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4315 - binary_accuracy: 0.8750\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4497 - binary_accuracy: 0.8652\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6540 - binary_accuracy: 0.8672\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9840 - binary_accuracy: 0.8809\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9980 - binary_accuracy: 0.8867\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7712 - binary_accuracy: 0.8828\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7136 - binary_accuracy: 0.8730\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0891 - binary_accuracy: 0.8730\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5456 - binary_accuracy: 0.8945\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7317 - binary_accuracy: 0.8926\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7504 - binary_accuracy: 0.8828\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5036 - binary_accuracy: 0.8730\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6405 - binary_accuracy: 0.8867\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3652 - binary_accuracy: 0.8945A: 0s - loss: 0.3817 - binary_accuracy: 0.9\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6185 - binary_accuracy: 0.8848\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9338 - binary_accuracy: 0.8770\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 1s 24ms/step - loss: 0.4069 - binary_accuracy: 0.8789\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5087 - binary_accuracy: 0.9023\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7168 - binary_accuracy: 0.8926\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8525 - binary_accuracy: 0.8965\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8778 - binary_accuracy: 0.8867\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.2014 - binary_accuracy: 0.8828\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.7539 - binary_accuracy: 0.8770\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6395 - binary_accuracy: 0.8809\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.8901 - binary_accuracy: 0.8848\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.8658 - binary_accuracy: 0.9043\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6186 - binary_accuracy: 0.9062\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5530 - binary_accuracy: 0.9023\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5773 - binary_accuracy: 0.8750\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.1095 - binary_accuracy: 0.8906\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5281 - binary_accuracy: 0.8984\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.3363 - binary_accuracy: 0.8965\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5952 - binary_accuracy: 0.8945\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7245 - binary_accuracy: 0.9043\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5661 - binary_accuracy: 0.9043\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.1470 - binary_accuracy: 0.9023\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.9429 - binary_accuracy: 0.8926\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.4657 - binary_accuracy: 0.9082\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 0.3325 - binary_accuracy: 0.8867\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6169 - binary_accuracy: 0.9160\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5546 - binary_accuracy: 0.9160\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3543 - binary_accuracy: 0.8906\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7936 - binary_accuracy: 0.9102\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3629 - binary_accuracy: 0.9062\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5776 - binary_accuracy: 0.9082\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.8675 - binary_accuracy: 0.9004\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7096 - binary_accuracy: 0.8926\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7450 - binary_accuracy: 0.8984\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4004 - binary_accuracy: 0.9102\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3336 - binary_accuracy: 0.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 8ms/step - loss: 1.8879 - binary_accuracy: 0.5723\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6915 - binary_accuracy: 0.6309\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5723 - binary_accuracy: 0.6953\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6316 - binary_accuracy: 0.6855\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5600 - binary_accuracy: 0.7383\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6477 - binary_accuracy: 0.7676\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5537 - binary_accuracy: 0.7520\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5441 - binary_accuracy: 0.7695\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6838 - binary_accuracy: 0.7812\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6978 - binary_accuracy: 0.7656\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5609 - binary_accuracy: 0.8047\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6189 - binary_accuracy: 0.7773\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6730 - binary_accuracy: 0.7852\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5261 - binary_accuracy: 0.8105\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4556 - binary_accuracy: 0.8281\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6091 - binary_accuracy: 0.8223\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.6342 - binary_accuracy: 0.8203\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4640 - binary_accuracy: 0.8340\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6008 - binary_accuracy: 0.8066\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5144 - binary_accuracy: 0.8457\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.4563 - binary_accuracy: 0.8262: 0s - loss: 0.4689 - binary_accuracy: 0.82\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.7340 - binary_accuracy: 0.8047\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4082 - binary_accuracy: 0.8438\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6090 - binary_accuracy: 0.8320A: 0s - loss: 0.7573 - binary_accuracy: 0.\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4508 - binary_accuracy: 0.8574\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4280 - binary_accuracy: 0.8477\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7183 - binary_accuracy: 0.8555\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0715 - binary_accuracy: 0.8477\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5165 - binary_accuracy: 0.8652\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4217 - binary_accuracy: 0.8633\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9762 - binary_accuracy: 0.8594\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5666 - binary_accuracy: 0.8574A: 0s - loss: 0.8016 - binary_accuracy: 0.\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9564 - binary_accuracy: 0.8516\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6198 - binary_accuracy: 0.8555\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5105 - binary_accuracy: 0.8789\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.1766 - binary_accuracy: 0.8496A: 0s - loss: 1.2088 - binary_accuracy: 0.8\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7550 - binary_accuracy: 0.8594\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9261 - binary_accuracy: 0.8770\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5195 - binary_accuracy: 0.8594\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5624 - binary_accuracy: 0.8887\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7786 - binary_accuracy: 0.8867\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6582 - binary_accuracy: 0.8789\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 10ms/step - loss: 1.7936 - binary_accuracy: 0.8828\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7773 - binary_accuracy: 0.8848\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5188 - binary_accuracy: 0.8730\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9903 - binary_accuracy: 0.8730\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.8339 - binary_accuracy: 0.8730\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.2101 - binary_accuracy: 0.8750\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7207 - binary_accuracy: 0.8672\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3272 - binary_accuracy: 0.9043\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7143 - binary_accuracy: 0.8809\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.5111 - binary_accuracy: 0.8633\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 2.0226 - binary_accuracy: 0.8711\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.5559 - binary_accuracy: 0.8867\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.0243 - binary_accuracy: 0.8809\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8666 - binary_accuracy: 0.8828\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3549 - binary_accuracy: 0.8750\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7051 - binary_accuracy: 0.8906\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.4181 - binary_accuracy: 0.8848A: 0s - loss: 2.0003 - binary_accuracy: 0.8\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8009 - binary_accuracy: 0.8867\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.7018 - binary_accuracy: 0.8867\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3601 - binary_accuracy: 0.8770\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.3204 - binary_accuracy: 0.8770\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.7372 - binary_accuracy: 0.8789\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.5989 - binary_accuracy: 0.8828\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.2219 - binary_accuracy: 0.8809\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9710 - binary_accuracy: 0.8945\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6364 - binary_accuracy: 0.9023\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.2126 - binary_accuracy: 0.8965\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7689 - binary_accuracy: 0.8828\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4362 - binary_accuracy: 0.8965\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7340 - binary_accuracy: 0.8906\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0598 - binary_accuracy: 0.8965\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3056 - binary_accuracy: 0.9121\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0856 - binary_accuracy: 0.9004\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.5346 - binary_accuracy: 0.9062\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9424 - binary_accuracy: 0.8828\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.4752 - binary_accuracy: 0.8789\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3106 - binary_accuracy: 0.8984A: 0s - loss: 0.2770 - binary_accuracy: 0.\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.5747 - binary_accuracy: 0.9102\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 2.2013 - binary_accuracy: 0.8906\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0881 - binary_accuracy: 0.9102\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4001 - binary_accuracy: 0.8945\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6422 - binary_accuracy: 0.9043\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.5321 - binary_accuracy: 0.9004\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6043 - binary_accuracy: 0.8984\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.4497 - binary_accuracy: 0.8809\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3777 - binary_accuracy: 0.9023\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8755 - binary_accuracy: 0.8945\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6528 - binary_accuracy: 0.9004\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7285 - binary_accuracy: 0.8926\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4796 - binary_accuracy: 0.9043\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3352 - binary_accuracy: 0.8770\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3773 - binary_accuracy: 0.9023\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6889 - binary_accuracy: 0.9043\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8603 - binary_accuracy: 0.9004\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.1654 - binary_accuracy: 0.8926\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 1.0147 - binary_accuracy: 0.8789\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0404 - binary_accuracy: 0.8945\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6311 - binary_accuracy: 0.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 6ms/step - loss: 1.0342 - binary_accuracy: 0.5879\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7327 - binary_accuracy: 0.6484\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6222 - binary_accuracy: 0.6484\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5876 - binary_accuracy: 0.6660\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6197 - binary_accuracy: 0.6855\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5815 - binary_accuracy: 0.6797\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5722 - binary_accuracy: 0.7031\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6430 - binary_accuracy: 0.7090\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5930 - binary_accuracy: 0.7383\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5555 - binary_accuracy: 0.7285\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5323 - binary_accuracy: 0.7539\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5427 - binary_accuracy: 0.7480\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5626 - binary_accuracy: 0.7441\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5797 - binary_accuracy: 0.7656\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5961 - binary_accuracy: 0.7734\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5507 - binary_accuracy: 0.7910\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5317 - binary_accuracy: 0.7910\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5899 - binary_accuracy: 0.7598\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6105 - binary_accuracy: 0.7441\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4870 - binary_accuracy: 0.7832\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6375 - binary_accuracy: 0.7988\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6483 - binary_accuracy: 0.7852\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6267 - binary_accuracy: 0.7871\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5701 - binary_accuracy: 0.8203\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5875 - binary_accuracy: 0.8164\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6380 - binary_accuracy: 0.8105\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5805 - binary_accuracy: 0.7969\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6200 - binary_accuracy: 0.8027\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6202 - binary_accuracy: 0.8047\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5344 - binary_accuracy: 0.8125\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4560 - binary_accuracy: 0.8184\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7167 - binary_accuracy: 0.8340\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6134 - binary_accuracy: 0.8281\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6371 - binary_accuracy: 0.8340\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8001 - binary_accuracy: 0.8301\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7445 - binary_accuracy: 0.8203\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5171 - binary_accuracy: 0.8477\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4758 - binary_accuracy: 0.8320\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6084 - binary_accuracy: 0.8320\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6317 - binary_accuracy: 0.8125\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4380 - binary_accuracy: 0.8516\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8128 - binary_accuracy: 0.8301\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6022 - binary_accuracy: 0.8203\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8007 - binary_accuracy: 0.8301\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9168 - binary_accuracy: 0.8574\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7031 - binary_accuracy: 0.8379\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7432 - binary_accuracy: 0.8477\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9161 - binary_accuracy: 0.8359\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8445 - binary_accuracy: 0.8262\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8063 - binary_accuracy: 0.8379\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7246 - binary_accuracy: 0.8379\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5991 - binary_accuracy: 0.8691\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6342 - binary_accuracy: 0.8652\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5708 - binary_accuracy: 0.8535\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8465 - binary_accuracy: 0.8477\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7091 - binary_accuracy: 0.8555\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8227 - binary_accuracy: 0.8574\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6036 - binary_accuracy: 0.8418\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6913 - binary_accuracy: 0.8633\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4393 - binary_accuracy: 0.8633\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8282 - binary_accuracy: 0.8594\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3656 - binary_accuracy: 0.8691\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.3528 - binary_accuracy: 0.8652\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5461 - binary_accuracy: 0.8672\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5779 - binary_accuracy: 0.8633\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6922 - binary_accuracy: 0.8477\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0244 - binary_accuracy: 0.8633A: 0s - loss: 1.3770 - binary_accuracy: 0.\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.2019 - binary_accuracy: 0.8691\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8274 - binary_accuracy: 0.8438\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5638 - binary_accuracy: 0.8359\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7812 - binary_accuracy: 0.8438\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5736 - binary_accuracy: 0.8496\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4797 - binary_accuracy: 0.8730\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 1.2346 - binary_accuracy: 0.850 - 0s 6ms/step - loss: 1.1752 - binary_accuracy: 0.8516\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6379 - binary_accuracy: 0.8770\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3491 - binary_accuracy: 0.8691\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5886 - binary_accuracy: 0.8770\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.1939 - binary_accuracy: 0.8691\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7795 - binary_accuracy: 0.8867\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7549 - binary_accuracy: 0.8477\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6382 - binary_accuracy: 0.8574\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3866 - binary_accuracy: 0.8613\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6479 - binary_accuracy: 0.8652\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5972 - binary_accuracy: 0.8633\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5576 - binary_accuracy: 0.8633\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6248 - binary_accuracy: 0.8652\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0807 - binary_accuracy: 0.8867\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 1.1207 - binary_accuracy: 0.8750\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6423 - binary_accuracy: 0.8574\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6103 - binary_accuracy: 0.8438\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7726 - binary_accuracy: 0.8652\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4598 - binary_accuracy: 0.8848\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.9838 - binary_accuracy: 0.8867\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.8003 - binary_accuracy: 0.8633\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7016 - binary_accuracy: 0.8730\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3986 - binary_accuracy: 0.8809\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.4896 - binary_accuracy: 0.8516\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.1496 - binary_accuracy: 0.8555\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6363 - binary_accuracy: 0.8789\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5222 - binary_accuracy: 0.8633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 7ms/step - loss: 1.0208 - binary_accuracy: 0.5859\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6841 - binary_accuracy: 0.6699\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6941 - binary_accuracy: 0.6602\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5861 - binary_accuracy: 0.7148\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5270 - binary_accuracy: 0.7617\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5276 - binary_accuracy: 0.7598\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5526 - binary_accuracy: 0.7305\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4973 - binary_accuracy: 0.7422\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5461 - binary_accuracy: 0.7539\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5139 - binary_accuracy: 0.7871\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5018 - binary_accuracy: 0.7656\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5073 - binary_accuracy: 0.7812\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5643 - binary_accuracy: 0.7676\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5311 - binary_accuracy: 0.7715\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4378 - binary_accuracy: 0.8086\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4797 - binary_accuracy: 0.8223A: 0s - loss: 0.3948 - binary_accuracy: 0\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4771 - binary_accuracy: 0.7812\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4759 - binary_accuracy: 0.8008\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4144 - binary_accuracy: 0.8203\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5056 - binary_accuracy: 0.7930\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4152 - binary_accuracy: 0.8086\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4123 - binary_accuracy: 0.8105\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4712 - binary_accuracy: 0.8184\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4080 - binary_accuracy: 0.8379\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4252 - binary_accuracy: 0.8262\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4449 - binary_accuracy: 0.8301\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4756 - binary_accuracy: 0.8125\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4483 - binary_accuracy: 0.8223\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3840 - binary_accuracy: 0.8555\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4434 - binary_accuracy: 0.8027\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4521 - binary_accuracy: 0.8008\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3767 - binary_accuracy: 0.8418\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3982 - binary_accuracy: 0.8340\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4165 - binary_accuracy: 0.8359\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4645 - binary_accuracy: 0.8359\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4133 - binary_accuracy: 0.8379\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3712 - binary_accuracy: 0.8652\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4677 - binary_accuracy: 0.8418\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3877 - binary_accuracy: 0.8574\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4514 - binary_accuracy: 0.8262\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3829 - binary_accuracy: 0.8457\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4585 - binary_accuracy: 0.8262\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3448 - binary_accuracy: 0.8574\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4275 - binary_accuracy: 0.8379\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3756 - binary_accuracy: 0.8496\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3888 - binary_accuracy: 0.8555\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4785 - binary_accuracy: 0.8535\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4932 - binary_accuracy: 0.8613\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4199 - binary_accuracy: 0.8262A: 0s - loss: 0.4313 - binary_accuracy: 0.826\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3563 - binary_accuracy: 0.8535\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4009 - binary_accuracy: 0.8516\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4030 - binary_accuracy: 0.8535A: 0s - loss: 0.2530 - binary_accuracy: 0\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.5981 - binary_accuracy: 0.8555\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.4377 - binary_accuracy: 0.8262\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3829 - binary_accuracy: 0.8613\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3842 - binary_accuracy: 0.8379\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3747 - binary_accuracy: 0.8457\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5022 - binary_accuracy: 0.8613\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5950 - binary_accuracy: 0.8594\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3538 - binary_accuracy: 0.8691\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3938 - binary_accuracy: 0.8516\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6571 - binary_accuracy: 0.8457\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4499 - binary_accuracy: 0.8555\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4628 - binary_accuracy: 0.8750\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9801 - binary_accuracy: 0.8496\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.4768 - binary_accuracy: 0.8691\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3793 - binary_accuracy: 0.8496\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.4690 - binary_accuracy: 0.8438\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3743 - binary_accuracy: 0.8594\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.5272 - binary_accuracy: 0.8613\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4461 - binary_accuracy: 0.8398\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5044 - binary_accuracy: 0.8457\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6661 - binary_accuracy: 0.8633\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5833 - binary_accuracy: 0.8535\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6092 - binary_accuracy: 0.8613\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5055 - binary_accuracy: 0.8672\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4719 - binary_accuracy: 0.8516\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9287 - binary_accuracy: 0.8379\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6372 - binary_accuracy: 0.8535\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5567 - binary_accuracy: 0.8672\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4392 - binary_accuracy: 0.8555\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5959 - binary_accuracy: 0.8652\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4682 - binary_accuracy: 0.8711\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5956 - binary_accuracy: 0.8418\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4778 - binary_accuracy: 0.8750A: 0s - loss: 0.4580 - binary_accuracy: 0.8\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5352 - binary_accuracy: 0.8613\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4941 - binary_accuracy: 0.8711\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4313 - binary_accuracy: 0.8496\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5477 - binary_accuracy: 0.8398\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3743 - binary_accuracy: 0.8398\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3465 - binary_accuracy: 0.8730\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5539 - binary_accuracy: 0.8555\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3601 - binary_accuracy: 0.8691\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8684 - binary_accuracy: 0.8457\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6224 - binary_accuracy: 0.8633\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5605 - binary_accuracy: 0.8867\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4310 - binary_accuracy: 0.8535\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3711 - binary_accuracy: 0.8516\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5354 - binary_accuracy: 0.8613\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6454 - binary_accuracy: 0.8633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 5ms/step - loss: 2.0187 - binary_accuracy: 0.5488\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7453 - binary_accuracy: 0.6191\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6677 - binary_accuracy: 0.6562\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6027 - binary_accuracy: 0.7031\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5677 - binary_accuracy: 0.7051\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5735 - binary_accuracy: 0.7129\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5687 - binary_accuracy: 0.7422\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5502 - binary_accuracy: 0.7637\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5218 - binary_accuracy: 0.7480\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4673 - binary_accuracy: 0.7734\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5771 - binary_accuracy: 0.7812\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5421 - binary_accuracy: 0.7852\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4570 - binary_accuracy: 0.8086\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5420 - binary_accuracy: 0.8027\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4613 - binary_accuracy: 0.7852\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5429 - binary_accuracy: 0.7930\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5597 - binary_accuracy: 0.7812\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5106 - binary_accuracy: 0.8125\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5846 - binary_accuracy: 0.7871\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5070 - binary_accuracy: 0.7969\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5327 - binary_accuracy: 0.7793\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4824 - binary_accuracy: 0.7930\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5547 - binary_accuracy: 0.7969\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5176 - binary_accuracy: 0.8125\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5001 - binary_accuracy: 0.7930\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5740 - binary_accuracy: 0.7969\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5468 - binary_accuracy: 0.8047\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4993 - binary_accuracy: 0.8027\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5276 - binary_accuracy: 0.8145\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5483 - binary_accuracy: 0.8105\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5914 - binary_accuracy: 0.8184\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.4569 - binary_accuracy: 0.8203\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4813 - binary_accuracy: 0.8066\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5183 - binary_accuracy: 0.8125\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5765 - binary_accuracy: 0.8203\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4539 - binary_accuracy: 0.8281\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5019 - binary_accuracy: 0.8184\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6045 - binary_accuracy: 0.7969\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4994 - binary_accuracy: 0.8262\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4551 - binary_accuracy: 0.8184\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4738 - binary_accuracy: 0.8164\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4184 - binary_accuracy: 0.8340\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4612 - binary_accuracy: 0.8438\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5906 - binary_accuracy: 0.8008\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5113 - binary_accuracy: 0.8066\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5162 - binary_accuracy: 0.8301\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4654 - binary_accuracy: 0.8281\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.5386 - binary_accuracy: 0.8145\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5323 - binary_accuracy: 0.8242\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4706 - binary_accuracy: 0.8281\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 1s 17ms/step - loss: 0.4788 - binary_accuracy: 0.8242\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5703 - binary_accuracy: 0.8145\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4851 - binary_accuracy: 0.8223\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5530 - binary_accuracy: 0.8008\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4427 - binary_accuracy: 0.8301\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4014 - binary_accuracy: 0.8438\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4540 - binary_accuracy: 0.8281\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3903 - binary_accuracy: 0.8379\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4948 - binary_accuracy: 0.8223\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4851 - binary_accuracy: 0.8418\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5160 - binary_accuracy: 0.8555\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4636 - binary_accuracy: 0.8457\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4524 - binary_accuracy: 0.8398\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4133 - binary_accuracy: 0.8301\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4191 - binary_accuracy: 0.8516\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3988 - binary_accuracy: 0.8477\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6506 - binary_accuracy: 0.8398\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5972 - binary_accuracy: 0.8320\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4808 - binary_accuracy: 0.8438\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5504 - binary_accuracy: 0.8281\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6097 - binary_accuracy: 0.8477\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5715 - binary_accuracy: 0.8457\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4895 - binary_accuracy: 0.8477\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.4854 - binary_accuracy: 0.8320\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4902 - binary_accuracy: 0.8438\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4125 - binary_accuracy: 0.8301\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6095 - binary_accuracy: 0.8145\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6671 - binary_accuracy: 0.8574\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7349 - binary_accuracy: 0.8281\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5737 - binary_accuracy: 0.8613\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7235 - binary_accuracy: 0.8457\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4582 - binary_accuracy: 0.8438\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7457 - binary_accuracy: 0.8477\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5294 - binary_accuracy: 0.8379\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4259 - binary_accuracy: 0.8613\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4884 - binary_accuracy: 0.8516\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6030 - binary_accuracy: 0.8359\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6270 - binary_accuracy: 0.8438\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6371 - binary_accuracy: 0.8691\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4366 - binary_accuracy: 0.8301\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3749 - binary_accuracy: 0.8574\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4022 - binary_accuracy: 0.8555\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6886 - binary_accuracy: 0.8438\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6331 - binary_accuracy: 0.8496\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5494 - binary_accuracy: 0.8301\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4066 - binary_accuracy: 0.8633\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4215 - binary_accuracy: 0.8535A: 0s - loss: 0.4314 - binary_accuracy: 0.8\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5316 - binary_accuracy: 0.8613\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5517 - binary_accuracy: 0.8594A: 0s - loss: 0.5204 - binary_accuracy: 0.8\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3693 - binary_accuracy: 0.8555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 6ms/step - loss: 1.4420 - binary_accuracy: 0.5801\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6869 - binary_accuracy: 0.6699\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6289 - binary_accuracy: 0.6816\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5525 - binary_accuracy: 0.6992\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5529 - binary_accuracy: 0.6816\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5527 - binary_accuracy: 0.7344\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5744 - binary_accuracy: 0.7266\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5531 - binary_accuracy: 0.7363\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5315 - binary_accuracy: 0.7344\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5754 - binary_accuracy: 0.7871\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4851 - binary_accuracy: 0.7930\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5343 - binary_accuracy: 0.8086\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5713 - binary_accuracy: 0.7754\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5209 - binary_accuracy: 0.8125\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5088 - binary_accuracy: 0.8125\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4589 - binary_accuracy: 0.8438\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.4619 - binary_accuracy: 0.8301\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4522 - binary_accuracy: 0.8262\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5279 - binary_accuracy: 0.8262\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3580 - binary_accuracy: 0.8750\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4731 - binary_accuracy: 0.8320\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4096 - binary_accuracy: 0.8594\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6747 - binary_accuracy: 0.8457\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7397 - binary_accuracy: 0.8418\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7866 - binary_accuracy: 0.8535\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9172 - binary_accuracy: 0.8301\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4521 - binary_accuracy: 0.8926\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4984 - binary_accuracy: 0.8848\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6756 - binary_accuracy: 0.8555\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4902 - binary_accuracy: 0.8457\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4643 - binary_accuracy: 0.8750\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5096 - binary_accuracy: 0.8867\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8531 - binary_accuracy: 0.8594\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6809 - binary_accuracy: 0.8809\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4086 - binary_accuracy: 0.8633\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4725 - binary_accuracy: 0.8887\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9546 - binary_accuracy: 0.8730\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7248 - binary_accuracy: 0.8770\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3294 - binary_accuracy: 0.8848\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5995 - binary_accuracy: 0.8848\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3717 - binary_accuracy: 0.8828\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6347 - binary_accuracy: 0.8809\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4682 - binary_accuracy: 0.8887\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.1087 - binary_accuracy: 0.8672\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4806 - binary_accuracy: 0.8809\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8067 - binary_accuracy: 0.8691\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.3979 - binary_accuracy: 0.8711\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8339 - binary_accuracy: 0.8887\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5474 - binary_accuracy: 0.8984\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9361 - binary_accuracy: 0.8789\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6677 - binary_accuracy: 0.8711\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8562 - binary_accuracy: 0.8926\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5324 - binary_accuracy: 0.8965\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.8815 - binary_accuracy: 0.8867\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4144 - binary_accuracy: 0.8789\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0541 - binary_accuracy: 0.8770\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.3370 - binary_accuracy: 0.8867\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.2609 - binary_accuracy: 0.8887\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0922 - binary_accuracy: 0.8691\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.3779 - binary_accuracy: 0.8828\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.3991 - binary_accuracy: 0.8750\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.3145 - binary_accuracy: 0.8730\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.0770 - binary_accuracy: 0.8887\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.3303 - binary_accuracy: 0.8848\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.5804 - binary_accuracy: 0.8867\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.2307 - binary_accuracy: 0.8809\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8267 - binary_accuracy: 0.8848\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.6445 - binary_accuracy: 0.8691\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4751 - binary_accuracy: 0.8926\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5359 - binary_accuracy: 0.8809\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5743 - binary_accuracy: 0.8730\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4105 - binary_accuracy: 0.9043\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9594 - binary_accuracy: 0.8906\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.1475 - binary_accuracy: 0.8867\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 2.0176 - binary_accuracy: 0.8750\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7730 - binary_accuracy: 0.8750\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9001 - binary_accuracy: 0.8711\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.2919 - binary_accuracy: 0.8848\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9263 - binary_accuracy: 0.8672\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.5184 - binary_accuracy: 0.8652\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.1076 - binary_accuracy: 0.8789\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7125 - binary_accuracy: 0.8984\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8989 - binary_accuracy: 0.8867\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6456 - binary_accuracy: 0.8809\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8486 - binary_accuracy: 0.8965\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.9535 - binary_accuracy: 0.8906\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0211 - binary_accuracy: 0.8809\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6452 - binary_accuracy: 0.8730\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0644 - binary_accuracy: 0.8887\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6936 - binary_accuracy: 0.8828\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6691 - binary_accuracy: 0.8828\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.8725 - binary_accuracy: 0.8867\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6334 - binary_accuracy: 0.8770\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9698 - binary_accuracy: 0.8848\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6755 - binary_accuracy: 0.8809\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8877 - binary_accuracy: 0.8594\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9831 - binary_accuracy: 0.8828\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.0443 - binary_accuracy: 0.8809\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8515 - binary_accuracy: 0.8555\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.1312 - binary_accuracy: 0.8770\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 5ms/step - loss: 1.5283 - binary_accuracy: 0.5302\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6394 - binary_accuracy: 0.6004\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5892 - binary_accuracy: 0.6608\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5418 - binary_accuracy: 0.6784\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5798 - binary_accuracy: 0.7115\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5737 - binary_accuracy: 0.7096\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5885 - binary_accuracy: 0.7271\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5894 - binary_accuracy: 0.7368\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5230 - binary_accuracy: 0.7973\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4938 - binary_accuracy: 0.7934\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5447 - binary_accuracy: 0.7953\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4917 - binary_accuracy: 0.7914\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.5215 - binary_accuracy: 0.806 - 0s 6ms/step - loss: 0.5155 - binary_accuracy: 0.7934\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5178 - binary_accuracy: 0.8246\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5318 - binary_accuracy: 0.8324\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6465 - binary_accuracy: 0.8109\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5014 - binary_accuracy: 0.8402\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4772 - binary_accuracy: 0.8558\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6456 - binary_accuracy: 0.8363\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4208 - binary_accuracy: 0.8499\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6077 - binary_accuracy: 0.8207\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6092 - binary_accuracy: 0.8343\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5332 - binary_accuracy: 0.8324\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5333 - binary_accuracy: 0.8402\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4369 - binary_accuracy: 0.8363\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4727 - binary_accuracy: 0.8713\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0280 - binary_accuracy: 0.8558\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5298 - binary_accuracy: 0.8421\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7473 - binary_accuracy: 0.8538\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5221 - binary_accuracy: 0.8694\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6703 - binary_accuracy: 0.8499\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5777 - binary_accuracy: 0.8558\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6239 - binary_accuracy: 0.8713\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3618 - binary_accuracy: 0.8674\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5360 - binary_accuracy: 0.8460\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7327 - binary_accuracy: 0.8499\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7260 - binary_accuracy: 0.8713\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6363 - binary_accuracy: 0.8382\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.1724 - binary_accuracy: 0.8402\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.7744 - binary_accuracy: 0.8538\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6329 - binary_accuracy: 0.8694\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9728 - binary_accuracy: 0.8402\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.1701 - binary_accuracy: 0.8538\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4375 - binary_accuracy: 0.8577\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7237 - binary_accuracy: 0.8674\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8475 - binary_accuracy: 0.8558\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7351 - binary_accuracy: 0.8519\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5134 - binary_accuracy: 0.8655\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6905 - binary_accuracy: 0.8635\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6317 - binary_accuracy: 0.8519\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5809 - binary_accuracy: 0.8713\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.4060 - binary_accuracy: 0.8499\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5866 - binary_accuracy: 0.8655\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.8787 - binary_accuracy: 0.8480\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5809 - binary_accuracy: 0.8519\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6159 - binary_accuracy: 0.8480\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.6643 - binary_accuracy: 0.8694\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8065 - binary_accuracy: 0.8460\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7367 - binary_accuracy: 0.8655\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.6867 - binary_accuracy: 0.8499\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7560 - binary_accuracy: 0.8577\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.8879 - binary_accuracy: 0.8791\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.5788 - binary_accuracy: 0.8499\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4230 - binary_accuracy: 0.8713\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.5454 - binary_accuracy: 0.8694\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.1561 - binary_accuracy: 0.8421\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7671 - binary_accuracy: 0.8830\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.2832 - binary_accuracy: 0.8752\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.1614 - binary_accuracy: 0.8655\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6322 - binary_accuracy: 0.8674\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5962 - binary_accuracy: 0.8713\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 1.2430 - binary_accuracy: 0.8577\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7285 - binary_accuracy: 0.8713\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.8277 - binary_accuracy: 0.8616\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.8105 - binary_accuracy: 0.8480\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.9450 - binary_accuracy: 0.9006\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.2417 - binary_accuracy: 0.8480\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4124 - binary_accuracy: 0.8947\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7717 - binary_accuracy: 0.8655\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 1.0434 - binary_accuracy: 0.8655\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7062 - binary_accuracy: 0.8811\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.1911 - binary_accuracy: 0.8772\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5402 - binary_accuracy: 0.8655\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.8543 - binary_accuracy: 0.8811\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.1162 - binary_accuracy: 0.8616\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.7890 - binary_accuracy: 0.8635\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5760 - binary_accuracy: 0.8616\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5622 - binary_accuracy: 0.8772\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.8255 - binary_accuracy: 0.8635\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4964 - binary_accuracy: 0.8830\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.0555 - binary_accuracy: 0.8733\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.6205 - binary_accuracy: 0.8694\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 1.0576 - binary_accuracy: 0.8713\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2874 - binary_accuracy: 0.8967\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.7890 - binary_accuracy: 0.8772\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.6531 - binary_accuracy: 0.8499\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9306 - binary_accuracy: 0.8830\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9592 - binary_accuracy: 0.8577\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.9087 - binary_accuracy: 0.8713\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1.2740 - binary_accuracy: 0.8616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "resultados_dropout = cross_val_score(\n",
    "    estimator= classificador_DropOut,\n",
    "    X = previsores,\n",
    "    y = classes,\n",
    "    cv = 10,\n",
    "    scoring = 'accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61403509, 0.92982456, 0.96491228, 0.84210526, 0.9122807 ,\n",
       "       0.92982456, 0.92982456, 0.9122807 , 0.84210526, 0.91071429])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8787907268170425"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_dropout.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09554043345014486"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_dropout.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede_Tuning(optimizer, loos, kernel_initializer, activation, neurons):\n",
    "    classificador3 = Sequential()\n",
    "    # Criando camada oculta\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = neurons,                  # Número de Neurônios da camada oculta\n",
    "            activation = activation,                   # Função de Ativação\n",
    "            kernel_initializer = kernel_initializer, # Kernel Inicial (random)\n",
    "            input_dim = qt_entradas                # Número de neurônios da camada de entrada\n",
    "        )\n",
    "    )\n",
    "\n",
    "    classificador3.add( \n",
    "        Dropout(0.2)\n",
    "    )\n",
    "\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = neurons,                  # Número de Neurônios da camada oculta\n",
    "            activation = activation,                   # Função de Ativação\n",
    "            kernel_initializer = kernel_initializer, # Kernel (random)\n",
    "        )\n",
    "    )\n",
    "    classificador3.add( \n",
    "        Dropout(0.2)\n",
    "    )\n",
    "        \n",
    "    # Criando camada de saída\n",
    "    classificador3.add( \n",
    "        Dense(\n",
    "            units = 1,                             # Número de Neurônios da saida\n",
    "            activation = 'sigmoid',                # Função de Ativação (sigmoide: ou 0 ou 1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    otimizador = tf.keras.optimizers.Adam(\n",
    "        lr =  0.001,    # Taxa de Aprendizagem\n",
    "        decay = 0.001,  # Decaimento\n",
    "        clipvalue = 0.5 # Evitar que fique preso\n",
    "    )\n",
    "\n",
    "    classificador3.compile(\n",
    "        optimizer= optimizer,           # Otimização da função de erro (Adam personalizado)\n",
    "        loss = loos,    # Maneira como calculamos o erro (neste caso entropia para binário)\n",
    "        metrics = ['binary_accuracy']    # Métricas \n",
    "    )  \n",
    "    \n",
    "    return classificador3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador = KerasClassifier(build_fn= criarRede_Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {'batch_size' : [10, 30],\n",
    "    'epochs' : [50, 30],\n",
    "    'optimizer' : ['adam', 'sgd'],\n",
    "    'loos': ['binary_crossentropy', 'hinge'],\n",
    "    'kernel_initializer': ['random_uniform', 'normal'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'neurons': [16, 8]    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 1s 7ms/step - loss: 1.3170 - binary_accuracy: 0.5670\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6258 - binary_accuracy: 0.6615\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5668 - binary_accuracy: 0.665 - 0s 4ms/step - loss: 0.5570 - binary_accuracy: 0.6703\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5373 - binary_accuracy: 0.6989\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.5332 - binary_accuracy: 0.6923\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5009 - binary_accuracy: 0.7275\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5005 - binary_accuracy: 0.7319\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4762 - binary_accuracy: 0.7319\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4648 - binary_accuracy: 0.7736\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4522 - binary_accuracy: 0.8022\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4468 - binary_accuracy: 0.8044\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.4255 - binary_accuracy: 0.8352\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.4382 - binary_accuracy: 0.8176\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4067 - binary_accuracy: 0.8484\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3934 - binary_accuracy: 0.8484\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3857 - binary_accuracy: 0.8593\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3742 - binary_accuracy: 0.8505\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3563 - binary_accuracy: 0.8418\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3314 - binary_accuracy: 0.8747\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2909 - binary_accuracy: 0.8945\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3076 - binary_accuracy: 0.9055\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2799 - binary_accuracy: 0.8945\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2741 - binary_accuracy: 0.8945\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2632 - binary_accuracy: 0.8901\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2639 - binary_accuracy: 0.8901\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2586 - binary_accuracy: 0.9033\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2368 - binary_accuracy: 0.9209\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2296 - binary_accuracy: 0.9121\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2052 - binary_accuracy: 0.9319\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.2192 - binary_accuracy: 0.9165\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2277 - binary_accuracy: 0.9121\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2294 - binary_accuracy: 0.9187\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2348 - binary_accuracy: 0.9011\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2175 - binary_accuracy: 0.9187\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2315 - binary_accuracy: 0.9165\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1958 - binary_accuracy: 0.9275\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2487 - binary_accuracy: 0.9011\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2025 - binary_accuracy: 0.9209\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1889 - binary_accuracy: 0.9341\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2289 - binary_accuracy: 0.9055\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2143 - binary_accuracy: 0.9187\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.1953 - binary_accuracy: 0.9297\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2196 - binary_accuracy: 0.9231\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1921 - binary_accuracy: 0.9297\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2495 - binary_accuracy: 0.9055\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2052 - binary_accuracy: 0.9253\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2030 - binary_accuracy: 0.9253\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1851 - binary_accuracy: 0.9297\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1841 - binary_accuracy: 0.9253\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2135 - binary_accuracy: 0.9099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 1s 5ms/step - loss: 1.0524 - binary_accuracy: 0.5956\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5906 - binary_accuracy: 0.6396\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5952 - binary_accuracy: 0.6242\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5585 - binary_accuracy: 0.6593\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5516 - binary_accuracy: 0.6593\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.5607 - binary_accuracy: 0.6462\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5228 - binary_accuracy: 0.7077\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4989 - binary_accuracy: 0.7341\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4880 - binary_accuracy: 0.7341\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4734 - binary_accuracy: 0.7451\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4543 - binary_accuracy: 0.7846\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4683 - binary_accuracy: 0.7802\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.4302 - binary_accuracy: 0.7956\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3986 - binary_accuracy: 0.8286\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4007 - binary_accuracy: 0.8066\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4165 - binary_accuracy: 0.8198\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3791 - binary_accuracy: 0.8462\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3692 - binary_accuracy: 0.8549\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3569 - binary_accuracy: 0.8615\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3696 - binary_accuracy: 0.8308\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3786 - binary_accuracy: 0.8242\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3434 - binary_accuracy: 0.8593\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3317 - binary_accuracy: 0.8659\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3108 - binary_accuracy: 0.8813\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3248 - binary_accuracy: 0.8637\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3546 - binary_accuracy: 0.8396\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3184 - binary_accuracy: 0.8703\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3053 - binary_accuracy: 0.8659\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3054 - binary_accuracy: 0.8703\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.3560 - binary_accuracy: 0.8374\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3551 - binary_accuracy: 0.8352\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3432 - binary_accuracy: 0.8527\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3184 - binary_accuracy: 0.8791\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3549 - binary_accuracy: 0.8418\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3551 - binary_accuracy: 0.8462\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3625 - binary_accuracy: 0.8308\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3314 - binary_accuracy: 0.8593A: 0s - loss: 0.3058 - binary_accuracy: 0.\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3571 - binary_accuracy: 0.8440\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3689 - binary_accuracy: 0.8352\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3516 - binary_accuracy: 0.8374\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3158 - binary_accuracy: 0.8571A: 0s - loss: 0.3109 - binary_accuracy: 0.86\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3219 - binary_accuracy: 0.8725\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3170 - binary_accuracy: 0.8725\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2989 - binary_accuracy: 0.8747\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3349 - binary_accuracy: 0.8571\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.3506 - binary_accuracy: 0.8374\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.3358 - binary_accuracy: 0.8527\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2913 - binary_accuracy: 0.8857\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3254 - binary_accuracy: 0.8615\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3098 - binary_accuracy: 0.8615\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 6ms/step - loss: 2.1549 - binary_accuracy: 0.5934\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.8117 - binary_accuracy: 0.5978\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6552 - binary_accuracy: 0.6000\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6290 - binary_accuracy: 0.6242\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6101 - binary_accuracy: 0.6637\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5570 - binary_accuracy: 0.6681\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5391 - binary_accuracy: 0.6703\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5246 - binary_accuracy: 0.6967\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.5035 - binary_accuracy: 0.7275\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4901 - binary_accuracy: 0.7209\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5099 - binary_accuracy: 0.7187\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4719 - binary_accuracy: 0.7473\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4734 - binary_accuracy: 0.7604\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4801 - binary_accuracy: 0.7429\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4400 - binary_accuracy: 0.7868\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4136 - binary_accuracy: 0.8066\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4222 - binary_accuracy: 0.8044\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3891 - binary_accuracy: 0.8330\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3978 - binary_accuracy: 0.8352\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3741 - binary_accuracy: 0.8440\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3662 - binary_accuracy: 0.8088\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3440 - binary_accuracy: 0.8681\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3466 - binary_accuracy: 0.8725\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3444 - binary_accuracy: 0.8484\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3147 - binary_accuracy: 0.8703\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3376 - binary_accuracy: 0.8681\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3571 - binary_accuracy: 0.8637\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3248 - binary_accuracy: 0.8659\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3042 - binary_accuracy: 0.8857\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3177 - binary_accuracy: 0.8813\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.2908 - binary_accuracy: 0.8945\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2605 - binary_accuracy: 0.8989\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2615 - binary_accuracy: 0.9055\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3088 - binary_accuracy: 0.8703\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2613 - binary_accuracy: 0.9143\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2676 - binary_accuracy: 0.9187\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3050 - binary_accuracy: 0.8725\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3058 - binary_accuracy: 0.8835\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2543 - binary_accuracy: 0.9077\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2759 - binary_accuracy: 0.8879\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2524 - binary_accuracy: 0.8989\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2251 - binary_accuracy: 0.9121\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2682 - binary_accuracy: 0.8879\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2743 - binary_accuracy: 0.8879\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2581 - binary_accuracy: 0.8901\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2638 - binary_accuracy: 0.8967\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2184 - binary_accuracy: 0.9055\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2378 - binary_accuracy: 0.9077\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2784 - binary_accuracy: 0.8901\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2615 - binary_accuracy: 0.8945\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 5ms/step - loss: 1.3153 - binary_accuracy: 0.5516\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.6759 - binary_accuracy: 0.6176\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5984 - binary_accuracy: 0.6549\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4743 - binary_accuracy: 0.7736\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5058 - binary_accuracy: 0.7407\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4834 - binary_accuracy: 0.7758\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.4212 - binary_accuracy: 0.7934\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4398 - binary_accuracy: 0.8000\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3992 - binary_accuracy: 0.8000\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3660 - binary_accuracy: 0.8330\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3935 - binary_accuracy: 0.8220\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3615 - binary_accuracy: 0.8549\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3400 - binary_accuracy: 0.8505\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3303 - binary_accuracy: 0.8462\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3291 - binary_accuracy: 0.8593\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3361 - binary_accuracy: 0.8527\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3472 - binary_accuracy: 0.8681\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3145 - binary_accuracy: 0.8747\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3178 - binary_accuracy: 0.8615\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3153 - binary_accuracy: 0.8681\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3066 - binary_accuracy: 0.8681\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3245 - binary_accuracy: 0.8725\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2791 - binary_accuracy: 0.9055\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2957 - binary_accuracy: 0.8769\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2930 - binary_accuracy: 0.8879\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2888 - binary_accuracy: 0.8901\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2900 - binary_accuracy: 0.8593\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2476 - binary_accuracy: 0.9165\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2922 - binary_accuracy: 0.8989\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2718 - binary_accuracy: 0.8901\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2709 - binary_accuracy: 0.8989\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2592 - binary_accuracy: 0.8901\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2593 - binary_accuracy: 0.8945\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2408 - binary_accuracy: 0.9011\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2613 - binary_accuracy: 0.8967\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2423 - binary_accuracy: 0.9077\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.2446 - binary_accuracy: 0.9165\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2940 - binary_accuracy: 0.8967\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2286 - binary_accuracy: 0.9143\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2296 - binary_accuracy: 0.9055\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2062 - binary_accuracy: 0.9253\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2460 - binary_accuracy: 0.9099\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2950 - binary_accuracy: 0.8681\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2482 - binary_accuracy: 0.8923\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2576 - binary_accuracy: 0.8945\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2507 - binary_accuracy: 0.9033\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2054 - binary_accuracy: 0.9231\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2253 - binary_accuracy: 0.8989\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2030 - binary_accuracy: 0.9121\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2567 - binary_accuracy: 0.9033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 1s 7ms/step - loss: 1.6292 - binary_accuracy: 0.5066\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.6433 - binary_accuracy: 0.5921\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6041 - binary_accuracy: 0.6316\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5623 - binary_accuracy: 0.6469\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4996 - binary_accuracy: 0.7237\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.4646 - binary_accuracy: 0.736 - 0s 5ms/step - loss: 0.4843 - binary_accuracy: 0.7325\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4789 - binary_accuracy: 0.7522\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4776 - binary_accuracy: 0.7390\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4332 - binary_accuracy: 0.8136\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4442 - binary_accuracy: 0.8070\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3974 - binary_accuracy: 0.8180\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3785 - binary_accuracy: 0.8377\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4080 - binary_accuracy: 0.8048\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3783 - binary_accuracy: 0.8575\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3577 - binary_accuracy: 0.8640\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3683 - binary_accuracy: 0.8640\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.3350 - binary_accuracy: 0.8443A: 0s - loss: 0.3234 - binary_accuracy: 0.8\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3480 - binary_accuracy: 0.8816\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3446 - binary_accuracy: 0.8443\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3559 - binary_accuracy: 0.8487\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3152 - binary_accuracy: 0.8772\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3351 - binary_accuracy: 0.8750\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3278 - binary_accuracy: 0.8596\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2963 - binary_accuracy: 0.8794\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2989 - binary_accuracy: 0.9013\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2715 - binary_accuracy: 0.8991\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2799 - binary_accuracy: 0.9013\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2802 - binary_accuracy: 0.9013\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.3028 - binary_accuracy: 0.8684\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2471 - binary_accuracy: 0.9035\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2525 - binary_accuracy: 0.9035\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2731 - binary_accuracy: 0.8772\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2476 - binary_accuracy: 0.9013\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2487 - binary_accuracy: 0.9035\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2361 - binary_accuracy: 0.9123\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2294 - binary_accuracy: 0.9123\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2429 - binary_accuracy: 0.9101\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.2100 - binary_accuracy: 0.9145\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2455 - binary_accuracy: 0.9057\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2612 - binary_accuracy: 0.8947\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2511 - binary_accuracy: 0.9013\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2104 - binary_accuracy: 0.9211\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2446 - binary_accuracy: 0.9057A: 0s - loss: 0.2277 - binary_accuracy: 0.\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2227 - binary_accuracy: 0.9145\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2468 - binary_accuracy: 0.8991\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1951 - binary_accuracy: 0.9167\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2713 - binary_accuracy: 0.8925\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2426 - binary_accuracy: 0.8991\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.2140 - binary_accuracy: 0.9145\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2256 - binary_accuracy: 0.9189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "c:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 1s 6ms/step - loss: 37.3806 - binary_accuracy: 0.6725\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1.0348 - binary_accuracy: 0.6835\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.8792 - binary_accuracy: 0.6813\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6581 - binary_accuracy: 0.6835\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6515 - binary_accuracy: 0.6835\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6461 - binary_accuracy: 0.6835\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6419 - binary_accuracy: 0.6835\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6384 - binary_accuracy: 0.6835\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6357 - binary_accuracy: 0.6835\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6334 - binary_accuracy: 0.6835\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6317 - binary_accuracy: 0.6835\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6303 - binary_accuracy: 0.6835\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.6325 - binary_accuracy: 0.6813\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6282 - binary_accuracy: 0.6835\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6275 - binary_accuracy: 0.6835\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6269 - binary_accuracy: 0.6835\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6264 - binary_accuracy: 0.6835\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6259 - binary_accuracy: 0.6835\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6256 - binary_accuracy: 0.6835\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6253 - binary_accuracy: 0.6835\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6251 - binary_accuracy: 0.6835\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6250 - binary_accuracy: 0.6835\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.6248 - binary_accuracy: 0.6835\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6248 - binary_accuracy: 0.6835\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6247 - binary_accuracy: 0.6835\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.6246 - binary_accuracy: 0.6835\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.6246 - binary_accuracy: 0.6835\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6245 - binary_accuracy: 0.6835\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6245 - binary_accuracy: 0.6835\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 0.6245 - binary_accuracy: 0.6835\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6210 - binary_accuracy: 0.688 - 0s 7ms/step - loss: 0.6244 - binary_accuracy: 0.6835\n",
      "Epoch 32/50\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 0.6712 - binary_accuracy: 0.6200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Pepe\\Projetos Python\\Curso Udemy DL\\Classificacao_Binaria.ipynb Célula: 67\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pepe/Projetos%20Python/Curso%20Udemy%20DL/Classificacao_Binaria.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pepe/Projetos%20Python/Curso%20Udemy%20DL/Classificacao_Binaria.ipynb#Y121sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     estimator \u001b[39m=\u001b[39m classificador,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pepe/Projetos%20Python/Curso%20Udemy%20DL/Classificacao_Binaria.ipynb#Y121sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     param_grid \u001b[39m=\u001b[39m parametros,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pepe/Projetos%20Python/Curso%20Udemy%20DL/Classificacao_Binaria.ipynb#Y121sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pepe/Projetos%20Python/Curso%20Udemy%20DL/Classificacao_Binaria.ipynb#Y121sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Pepe/Projetos%20Python/Curso%20Udemy%20DL/Classificacao_Binaria.ipynb#Y121sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m grid_search \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39;49mfit(previsores, classes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Pepe/Projetos%20Python/Curso%20Udemy%20DL/Classificacao_Binaria.ipynb#Y121sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m melhores_parametros \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Pepe/Projetos%20Python/Curso%20Udemy%20DL/Classificacao_Binaria.ipynb#Y121sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m melhor_precisao \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_score_\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1374\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:220\u001b[0m, in \u001b[0;36mKerasClassifier.fit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid shape for y: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(y\u001b[39m.\u001b[39mshape))\n\u001b[0;32m    219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_)\n\u001b[1;32m--> 220\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(KerasClassifier, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfit(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:163\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m fit_args \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_sk_params(Sequential\u001b[39m.\u001b[39mfit))\n\u001b[0;32m    161\u001b[0m fit_args\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m--> 163\u001b[0m history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(x, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_args)\n\u001b[0;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1193\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1187\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1188\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1189\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1190\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1191\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1192\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1193\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1194\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1195\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Pepe\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator = classificador,\n",
    "    param_grid = parametros,\n",
    "    scoring = 'accuracy',\n",
    "    cv = 5,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "grid_search = grid_search.fit(previsores, classes)\n",
    "\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_precisao = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SALVAR A REDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 1.8155 - binary_accuracy: 0.5114\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.7016 - binary_accuracy: 0.5870\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.6174 - binary_accuracy: 0.6274\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.5873 - binary_accuracy: 0.6995\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.5656 - binary_accuracy: 0.6942\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.5291 - binary_accuracy: 0.7293\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.5313 - binary_accuracy: 0.7223\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.5270 - binary_accuracy: 0.7452\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4923 - binary_accuracy: 0.7469\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4562 - binary_accuracy: 0.7891\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4569 - binary_accuracy: 0.7979\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4347 - binary_accuracy: 0.7838\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4110 - binary_accuracy: 0.8155\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3995 - binary_accuracy: 0.8278\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3996 - binary_accuracy: 0.8295\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3750 - binary_accuracy: 0.8383\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3794 - binary_accuracy: 0.8313\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4003 - binary_accuracy: 0.8225\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3815 - binary_accuracy: 0.8436\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3699 - binary_accuracy: 0.8383\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.3532 - binary_accuracy: 0.8576\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3526 - binary_accuracy: 0.8612\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3322 - binary_accuracy: 0.8453\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3316 - binary_accuracy: 0.8576\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3332 - binary_accuracy: 0.8629\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3224 - binary_accuracy: 0.8682\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3212 - binary_accuracy: 0.8752\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2941 - binary_accuracy: 0.8928\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2990 - binary_accuracy: 0.8752\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2725 - binary_accuracy: 0.8875\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2742 - binary_accuracy: 0.8893\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.3033 - binary_accuracy: 0.8752\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3040 - binary_accuracy: 0.8576\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2952 - binary_accuracy: 0.8875\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3091 - binary_accuracy: 0.8647\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3032 - binary_accuracy: 0.8752\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2656 - binary_accuracy: 0.8910\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2862 - binary_accuracy: 0.8981\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3251 - binary_accuracy: 0.8612\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2667 - binary_accuracy: 0.8946\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2736 - binary_accuracy: 0.8946\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2776 - binary_accuracy: 0.8981\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.3297 - binary_accuracy: 0.8717A: 0s - loss: 0.3669 - binary_accuracy: 0.\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2865 - binary_accuracy: 0.8840\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2422 - binary_accuracy: 0.9121\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2723 - binary_accuracy: 0.8840\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2569 - binary_accuracy: 0.8981\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2664 - binary_accuracy: 0.9033\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2495 - binary_accuracy: 0.9069\n",
      "Epoch 50/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2674 - binary_accuracy: 0.8946\n",
      "Epoch 51/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2965 - binary_accuracy: 0.8963\n",
      "Epoch 52/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2504 - binary_accuracy: 0.9139\n",
      "Epoch 53/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2534 - binary_accuracy: 0.8998\n",
      "Epoch 54/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2659 - binary_accuracy: 0.9033\n",
      "Epoch 55/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2350 - binary_accuracy: 0.9069\n",
      "Epoch 56/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2290 - binary_accuracy: 0.9086\n",
      "Epoch 57/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2932 - binary_accuracy: 0.8840\n",
      "Epoch 58/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2305 - binary_accuracy: 0.9086A: 0s - loss: 0.1893 - binary_accuracy: 0.\n",
      "Epoch 59/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2593 - binary_accuracy: 0.8875\n",
      "Epoch 60/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2233 - binary_accuracy: 0.9156\n",
      "Epoch 61/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2263 - binary_accuracy: 0.9051\n",
      "Epoch 62/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2604 - binary_accuracy: 0.9016\n",
      "Epoch 63/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2347 - binary_accuracy: 0.9086\n",
      "Epoch 64/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2349 - binary_accuracy: 0.9121\n",
      "Epoch 65/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2613 - binary_accuracy: 0.8840\n",
      "Epoch 66/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2069 - binary_accuracy: 0.9192\n",
      "Epoch 67/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2497 - binary_accuracy: 0.9051\n",
      "Epoch 68/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2373 - binary_accuracy: 0.9051A: 0s - loss: 0.2677 - binary_accuracy: 0\n",
      "Epoch 69/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2360 - binary_accuracy: 0.9033\n",
      "Epoch 70/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2458 - binary_accuracy: 0.8981\n",
      "Epoch 71/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2610 - binary_accuracy: 0.8928\n",
      "Epoch 72/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2804 - binary_accuracy: 0.8822\n",
      "Epoch 73/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2403 - binary_accuracy: 0.9033\n",
      "Epoch 74/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2270 - binary_accuracy: 0.9121\n",
      "Epoch 75/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2156 - binary_accuracy: 0.9244\n",
      "Epoch 76/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2411 - binary_accuracy: 0.9174\n",
      "Epoch 77/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2434 - binary_accuracy: 0.8981\n",
      "Epoch 78/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2065 - binary_accuracy: 0.9174\n",
      "Epoch 79/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2415 - binary_accuracy: 0.9051\n",
      "Epoch 80/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2270 - binary_accuracy: 0.9051\n",
      "Epoch 81/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2447 - binary_accuracy: 0.9033\n",
      "Epoch 82/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2327 - binary_accuracy: 0.8963\n",
      "Epoch 83/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2515 - binary_accuracy: 0.9121\n",
      "Epoch 84/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2319 - binary_accuracy: 0.8998\n",
      "Epoch 85/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2361 - binary_accuracy: 0.9033A: 0s - loss: 0.2078 - binary_accuracy: 0.\n",
      "Epoch 86/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2317 - binary_accuracy: 0.9209\n",
      "Epoch 87/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2487 - binary_accuracy: 0.8981\n",
      "Epoch 88/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2816 - binary_accuracy: 0.8946\n",
      "Epoch 89/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2245 - binary_accuracy: 0.9156\n",
      "Epoch 90/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2297 - binary_accuracy: 0.9069\n",
      "Epoch 91/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2289 - binary_accuracy: 0.9121\n",
      "Epoch 92/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2143 - binary_accuracy: 0.9139\n",
      "Epoch 93/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2389 - binary_accuracy: 0.9104\n",
      "Epoch 94/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2235 - binary_accuracy: 0.9139\n",
      "Epoch 95/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2191 - binary_accuracy: 0.9139\n",
      "Epoch 96/100\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2438 - binary_accuracy: 0.9016\n",
      "Epoch 97/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2304 - binary_accuracy: 0.9139\n",
      "Epoch 98/100\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.2463 - binary_accuracy: 0.8946\n",
      "Epoch 99/100\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.2296 - binary_accuracy: 0.9121A: 0s - loss: 0.1303 - binary_accuracy\n",
      "Epoch 100/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2167 - binary_accuracy: 0.9192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29276b8b9a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador = Sequential()\n",
    "classificador.add(\n",
    "    Dense(\n",
    "        units = 8,\n",
    "        activation = 'relu',\n",
    "        kernel_initializer= 'normal',\n",
    "        input_dim = 30\n",
    "    )\n",
    ")\n",
    "classificador.add(\n",
    "    Dropout(0.2)\n",
    ")\n",
    "classificador.add(\n",
    "    Dense(\n",
    "        units = 8,\n",
    "        activation = 'relu',\n",
    "        kernel_initializer= 'normal'\n",
    "    )\n",
    ")\n",
    "classificador.add(\n",
    "    Dropout(0.2)\n",
    ")\n",
    "classificador.add(\n",
    "    Dense(\n",
    "        units = 1,\n",
    "        activation = 'sigmoid',\n",
    "    )\n",
    ")\n",
    "classificador.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['binary_accuracy']\n",
    ")\n",
    "classificador.fit(previsores,classes, batch_size= 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_json = classificador.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classificador_breast.json', 'w') as json_file:\n",
    "    json_file.write(classificador_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador.save_weights('classificador_breast.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar Rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tem\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "if os.path.isfile('dados/autos.csv'):\n",
    "    print('tem')\n",
    "else:\n",
    "    print('não tem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo = open('classificador_breast.json', 'r')\n",
    "estrutura_rede = arquivo.read()\n",
    "arquivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador = model_from_json(estrutura_rede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador.load_weights('classificador_breast.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.83267009e-01],\n",
       "       [4.67442063e-04],\n",
       "       [8.84694874e-01],\n",
       "       [5.49451351e-01],\n",
       "       [8.06275129e-01],\n",
       "       [9.90745425e-01],\n",
       "       [2.55757663e-02],\n",
       "       [8.41319621e-01],\n",
       "       [1.00000000e+00],\n",
       "       [6.92486879e-04],\n",
       "       [3.89976300e-08],\n",
       "       [8.88862967e-01],\n",
       "       [8.96586418e-01],\n",
       "       [3.90684372e-03],\n",
       "       [5.32469749e-01],\n",
       "       [2.96018940e-14],\n",
       "       [6.29198313e-01],\n",
       "       [8.34134519e-01],\n",
       "       [4.98166628e-05],\n",
       "       [7.08237421e-05],\n",
       "       [9.09784675e-01],\n",
       "       [9.59440827e-01],\n",
       "       [6.11389697e-01],\n",
       "       [8.84066582e-01],\n",
       "       [9.99992609e-01],\n",
       "       [7.56722400e-08],\n",
       "       [8.85526180e-01],\n",
       "       [1.00000000e+00],\n",
       "       [1.46081002e-05],\n",
       "       [9.88244891e-01],\n",
       "       [3.54236156e-01],\n",
       "       [2.25875740e-09],\n",
       "       [9.99998927e-01],\n",
       "       [9.44812417e-01],\n",
       "       [9.66842949e-01],\n",
       "       [2.03049231e-12],\n",
       "       [3.53359268e-04],\n",
       "       [1.00000000e+00],\n",
       "       [8.88862967e-01],\n",
       "       [8.73629391e-01],\n",
       "       [1.46828620e-02],\n",
       "       [1.00000000e+00],\n",
       "       [1.41564891e-01],\n",
       "       [9.89586711e-01],\n",
       "       [9.73332644e-01],\n",
       "       [8.88862967e-01],\n",
       "       [5.49250864e-04],\n",
       "       [1.39101292e-03],\n",
       "       [8.34475935e-01],\n",
       "       [9.89880979e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.88862967e-01],\n",
       "       [9.95264173e-01],\n",
       "       [9.99998093e-01],\n",
       "       [1.02431250e-05],\n",
       "       [9.81477857e-01],\n",
       "       [9.99649286e-01],\n",
       "       [8.88862967e-01],\n",
       "       [1.86123881e-08],\n",
       "       [6.99457943e-01],\n",
       "       [3.02038838e-10],\n",
       "       [9.46380198e-01],\n",
       "       [9.40375149e-01],\n",
       "       [8.55783343e-01],\n",
       "       [8.88044108e-03],\n",
       "       [8.88862967e-01],\n",
       "       [9.50768530e-01],\n",
       "       [1.00000000e+00],\n",
       "       [1.83143452e-01],\n",
       "       [5.06446871e-04],\n",
       "       [4.54388618e-01],\n",
       "       [6.85921591e-03],\n",
       "       [4.01057005e-01],\n",
       "       [8.38622808e-01],\n",
       "       [8.91643703e-01],\n",
       "       [9.99725997e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.40187216e-01],\n",
       "       [2.89070301e-09],\n",
       "       [2.51298845e-02],\n",
       "       [9.99999762e-01],\n",
       "       [9.42319274e-01],\n",
       "       [9.91564393e-01],\n",
       "       [9.35817411e-07],\n",
       "       [9.91546452e-01],\n",
       "       [9.97995377e-01],\n",
       "       [8.88862967e-01],\n",
       "       [2.21576158e-07],\n",
       "       [7.57441223e-01],\n",
       "       [1.20709873e-10],\n",
       "       [4.05907743e-02],\n",
       "       [9.96640801e-01],\n",
       "       [7.94411719e-01],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [8.88862967e-01],\n",
       "       [9.99952078e-01],\n",
       "       [1.72670681e-07],\n",
       "       [3.49026530e-10],\n",
       "       [9.55445290e-01],\n",
       "       [9.99049604e-01],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [6.77908897e-01],\n",
       "       [2.84582615e-01],\n",
       "       [7.34045985e-04],\n",
       "       [5.35264844e-05],\n",
       "       [8.05536256e-05],\n",
       "       [2.02025976e-02],\n",
       "       [1.04892510e-11],\n",
       "       [1.00000000e+00],\n",
       "       [9.13077950e-01],\n",
       "       [3.59338880e-01],\n",
       "       [2.39947083e-04],\n",
       "       [8.95727038e-01],\n",
       "       [8.88862967e-01],\n",
       "       [1.51074901e-01],\n",
       "       [3.24588655e-05],\n",
       "       [1.05617149e-03],\n",
       "       [6.59928977e-01],\n",
       "       [1.15825921e-01],\n",
       "       [9.37640786e-01],\n",
       "       [2.46991590e-01],\n",
       "       [8.88862967e-01],\n",
       "       [5.25746830e-02],\n",
       "       [9.06529725e-01],\n",
       "       [9.25885201e-01],\n",
       "       [7.04356655e-03],\n",
       "       [9.99673486e-01],\n",
       "       [9.42017972e-01],\n",
       "       [1.31157060e-06],\n",
       "       [9.98203993e-01],\n",
       "       [9.49511588e-01],\n",
       "       [8.88862967e-01],\n",
       "       [5.07447403e-05],\n",
       "       [8.43257129e-01],\n",
       "       [5.28739884e-06],\n",
       "       [1.27958643e-04],\n",
       "       [8.88862967e-01],\n",
       "       [1.71106890e-01],\n",
       "       [9.87269640e-01],\n",
       "       [6.35507643e-01],\n",
       "       [4.66225947e-06],\n",
       "       [1.72790080e-01],\n",
       "       [6.98396623e-01],\n",
       "       [9.80521083e-01],\n",
       "       [8.88696194e-01],\n",
       "       [9.99994993e-01],\n",
       "       [8.88862967e-01],\n",
       "       [1.71191597e-04],\n",
       "       [9.98650849e-01],\n",
       "       [2.98888117e-01],\n",
       "       [8.23662817e-01],\n",
       "       [5.05043380e-02],\n",
       "       [9.20875847e-01],\n",
       "       [5.64833317e-05],\n",
       "       [8.59830141e-01],\n",
       "       [8.88862967e-01],\n",
       "       [1.20314406e-04],\n",
       "       [1.00000000e+00],\n",
       "       [8.41467857e-01],\n",
       "       [7.10495636e-02],\n",
       "       [1.09551160e-10],\n",
       "       [5.26543800e-03],\n",
       "       [9.90831554e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.88862967e-01],\n",
       "       [9.87359464e-01],\n",
       "       [8.34646761e-01],\n",
       "       [9.88149166e-01],\n",
       "       [8.61593008e-01],\n",
       "       [2.77647153e-02],\n",
       "       [1.49190307e-01],\n",
       "       [2.08417848e-02],\n",
       "       [7.93810487e-02],\n",
       "       [6.60798371e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.92919981e-01],\n",
       "       [8.82536948e-01],\n",
       "       [6.38352867e-05],\n",
       "       [9.98761415e-01],\n",
       "       [1.19383105e-06],\n",
       "       [1.19280186e-09],\n",
       "       [7.74562452e-03],\n",
       "       [1.00000000e+00],\n",
       "       [8.88862967e-01],\n",
       "       [8.88862967e-01],\n",
       "       [4.18043166e-01],\n",
       "       [9.95643301e-09],\n",
       "       [8.15186941e-04],\n",
       "       [1.00000000e+00],\n",
       "       [9.99986529e-01],\n",
       "       [2.69106627e-01],\n",
       "       [1.05614411e-02],\n",
       "       [1.00000000e+00],\n",
       "       [8.66323173e-01],\n",
       "       [1.75022453e-01],\n",
       "       [1.67468227e-02],\n",
       "       [8.86855960e-01],\n",
       "       [1.00000000e+00],\n",
       "       [8.88862967e-01],\n",
       "       [9.44116175e-01],\n",
       "       [8.15320108e-03],\n",
       "       [1.17067038e-05],\n",
       "       [4.50144071e-05],\n",
       "       [1.25477463e-01],\n",
       "       [5.71630746e-02],\n",
       "       [3.87261845e-02],\n",
       "       [7.04758763e-01],\n",
       "       [9.69530523e-01],\n",
       "       [9.86238480e-01],\n",
       "       [3.92889284e-17],\n",
       "       [6.84238732e-01],\n",
       "       [9.81795013e-01],\n",
       "       [9.87300754e-01],\n",
       "       [9.54365909e-01],\n",
       "       [1.69898430e-03],\n",
       "       [6.59177244e-01],\n",
       "       [9.98099148e-01],\n",
       "       [1.00000000e+00],\n",
       "       [8.88862967e-01],\n",
       "       [8.40494990e-01],\n",
       "       [9.99871492e-01],\n",
       "       [7.44170102e-05],\n",
       "       [9.95919704e-01],\n",
       "       [9.74366188e-01],\n",
       "       [7.87542522e-05],\n",
       "       [9.86946404e-01],\n",
       "       [9.83622372e-01],\n",
       "       [9.21203375e-01],\n",
       "       [9.99996305e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.98434818e-01],\n",
       "       [1.06055789e-13],\n",
       "       [4.84328866e-01],\n",
       "       [8.88862967e-01],\n",
       "       [3.16799444e-04],\n",
       "       [9.99249279e-01],\n",
       "       [1.01354502e-01],\n",
       "       [8.78324151e-01],\n",
       "       [1.88447163e-03],\n",
       "       [1.00000000e+00],\n",
       "       [2.35247967e-06],\n",
       "       [1.00000000e+00],\n",
       "       [9.33618486e-01],\n",
       "       [9.99955058e-01],\n",
       "       [6.43936455e-01],\n",
       "       [8.88862967e-01],\n",
       "       [1.15667162e-02],\n",
       "       [8.39432776e-01],\n",
       "       [9.97649372e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.83664072e-01],\n",
       "       [9.96660113e-01],\n",
       "       [9.50181186e-01],\n",
       "       [1.08075000e-01],\n",
       "       [8.49205136e-01],\n",
       "       [7.29694009e-01],\n",
       "       [6.84604418e-09],\n",
       "       [9.99655724e-01],\n",
       "       [4.73109866e-03],\n",
       "       [9.26722646e-01],\n",
       "       [2.09088132e-04],\n",
       "       [9.79885757e-01],\n",
       "       [5.55852021e-04],\n",
       "       [9.79357436e-02],\n",
       "       [5.83717925e-03],\n",
       "       [1.30560920e-01],\n",
       "       [5.92279613e-01],\n",
       "       [6.36167551e-12],\n",
       "       [9.96332526e-01],\n",
       "       [9.73465919e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.88862967e-01],\n",
       "       [5.25685489e-01],\n",
       "       [8.79322827e-01],\n",
       "       [9.88475204e-01],\n",
       "       [1.19015342e-02],\n",
       "       [1.66770928e-02],\n",
       "       [1.00000000e+00],\n",
       "       [8.79899383e-01],\n",
       "       [9.99251068e-01],\n",
       "       [9.95988548e-01],\n",
       "       [9.99822080e-01],\n",
       "       [5.71259576e-12],\n",
       "       [4.22515273e-01],\n",
       "       [9.99984503e-01],\n",
       "       [8.88702154e-01],\n",
       "       [3.04932598e-07],\n",
       "       [2.40679697e-07],\n",
       "       [9.90784883e-01],\n",
       "       [1.00000000e+00],\n",
       "       [2.23099627e-03],\n",
       "       [4.88853812e-01],\n",
       "       [1.94633994e-05],\n",
       "       [9.78112439e-05],\n",
       "       [8.84381235e-01],\n",
       "       [9.99267876e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.88862967e-01],\n",
       "       [9.47585881e-01],\n",
       "       [1.04410377e-04],\n",
       "       [9.90571499e-01],\n",
       "       [9.99113619e-01],\n",
       "       [1.83367883e-05],\n",
       "       [1.00000000e+00],\n",
       "       [5.23609877e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.88862967e-01],\n",
       "       [4.68155146e-01],\n",
       "       [8.65143776e-01],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [9.21094239e-01],\n",
       "       [8.88862967e-01],\n",
       "       [3.30795825e-01],\n",
       "       [4.25606482e-02],\n",
       "       [9.97089088e-01],\n",
       "       [4.06406075e-01],\n",
       "       [3.87105167e-01],\n",
       "       [8.83029759e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.89720976e-01],\n",
       "       [7.03883316e-08],\n",
       "       [6.69790898e-03],\n",
       "       [1.00000000e+00],\n",
       "       [8.63024533e-01],\n",
       "       [8.88862967e-01],\n",
       "       [9.68337357e-01],\n",
       "       [8.78901919e-05],\n",
       "       [8.59298468e-01],\n",
       "       [9.69699954e-07],\n",
       "       [9.98553813e-01],\n",
       "       [1.04613027e-10],\n",
       "       [6.57018900e-01],\n",
       "       [2.15036217e-02],\n",
       "       [9.89329755e-01],\n",
       "       [1.07306195e-03],\n",
       "       [2.35743482e-05],\n",
       "       [1.00000000e+00],\n",
       "       [2.28059250e-07],\n",
       "       [9.78584707e-01],\n",
       "       [8.92304182e-01],\n",
       "       [8.88862967e-01],\n",
       "       [9.58844543e-01],\n",
       "       [9.42191601e-01],\n",
       "       [8.88862967e-01],\n",
       "       [3.59524856e-05],\n",
       "       [9.85063553e-01],\n",
       "       [9.87999737e-01],\n",
       "       [4.63074213e-03],\n",
       "       [9.41063285e-01],\n",
       "       [9.91266310e-01],\n",
       "       [8.88862967e-01],\n",
       "       [9.88486290e-01],\n",
       "       [6.94185913e-01],\n",
       "       [9.65244830e-01],\n",
       "       [9.66034532e-01],\n",
       "       [7.76774943e-01],\n",
       "       [9.88287508e-01],\n",
       "       [6.89083278e-01],\n",
       "       [7.79076479e-04],\n",
       "       [9.45815206e-01],\n",
       "       [9.10069585e-01],\n",
       "       [8.45853627e-01],\n",
       "       [2.28696104e-13],\n",
       "       [7.05794632e-01],\n",
       "       [3.30295771e-01],\n",
       "       [9.95945632e-01],\n",
       "       [8.77698958e-01],\n",
       "       [1.00000000e+00],\n",
       "       [3.78749728e-01],\n",
       "       [2.36947715e-04],\n",
       "       [8.53309274e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.66783738e-01],\n",
       "       [8.62688899e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.73164296e-01],\n",
       "       [8.88862967e-01],\n",
       "       [2.02255615e-05],\n",
       "       [5.73263407e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.88862967e-01],\n",
       "       [4.72297934e-09],\n",
       "       [9.99980211e-01],\n",
       "       [3.47204943e-04],\n",
       "       [1.00000000e+00],\n",
       "       [2.17132947e-05],\n",
       "       [1.44601226e-01],\n",
       "       [8.88862967e-01],\n",
       "       [8.88862967e-01],\n",
       "       [9.75919843e-01],\n",
       "       [9.83317614e-01],\n",
       "       [8.51966619e-01],\n",
       "       [9.29169595e-01],\n",
       "       [8.76251757e-01],\n",
       "       [1.08583902e-20],\n",
       "       [8.87279894e-05],\n",
       "       [1.33519992e-03],\n",
       "       [9.96136010e-01],\n",
       "       [8.88862967e-01],\n",
       "       [7.11649184e-09],\n",
       "       [9.27653670e-01],\n",
       "       [3.63578934e-20],\n",
       "       [8.16546619e-01],\n",
       "       [5.26503086e-01],\n",
       "       [2.09688660e-04],\n",
       "       [9.90960121e-01],\n",
       "       [9.99986291e-01],\n",
       "       [1.44515946e-06],\n",
       "       [1.87134534e-01],\n",
       "       [9.99552429e-01],\n",
       "       [4.97971684e-01],\n",
       "       [2.72462040e-01],\n",
       "       [9.89150345e-01],\n",
       "       [9.95997190e-01],\n",
       "       [8.70658219e-01],\n",
       "       [6.66603684e-01],\n",
       "       [4.46360111e-01],\n",
       "       [4.39667003e-03],\n",
       "       [5.89902754e-07],\n",
       "       [5.21714574e-05],\n",
       "       [9.99678612e-01],\n",
       "       [2.67758296e-08]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = classificador.predict(previsores_treinamento)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador.compile(\n",
    "    loss= 'binary_crossentropy', \n",
    "    optimizer= 'adam', \n",
    "    metrics= 'accuracy'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 11ms/step - loss: 0.1505 - accuracy: 0.9438\n"
     ]
    }
   ],
   "source": [
    "resultado = classificador.evaluate(previsores, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15051181614398956, 0.9437609910964966]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc79cfbc3d7d6651cddcc55ca4b88c642454132f40897641b9ee5bd9e8ac650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
